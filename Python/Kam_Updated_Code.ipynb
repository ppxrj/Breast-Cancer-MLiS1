{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ppxrj/Breast-Cancer-MLiS1/blob/main/Kam_Updated_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c61ce62",
      "metadata": {
        "id": "9c61ce62",
        "outputId": "c92a32f4-4273-4421-865d-162b3367d1c1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "%pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f707fba",
      "metadata": {
        "id": "0f707fba"
      },
      "source": [
        "DATA INGESTION in pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47fd7c54",
      "metadata": {
        "id": "47fd7c54",
        "outputId": "277185df-09a6-4745-c565-064cb9eefcc0"
      },
      "outputs": [],
      "source": [
        "#Dataset\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "cancer_data = fetch_ucirepo(id=17) # fetch dataset\n",
        "# data (as pandas dataframes)\n",
        "X = cancer_data.data.features\n",
        "y = cancer_data.data.targets\n",
        "ids= cancer_data.data.ids\n",
        "\n",
        "# Check the shape of data\n",
        "print(f\"Features shape: {X.shape}\") #Features shape: (699, 9)\n",
        "print(f\"Target shape: {y.shape}\") #Target shape: (699, 1)\n",
        "print(f\"IDs shape: {ids.shape}\") #IDs shape: (699, 1)\n",
        "# Look at data\n",
        "print(X.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a14f4c72",
      "metadata": {
        "id": "a14f4c72",
        "outputId": "3fc1b327-7b13-46b9-ba22-7c1221184a8f"
      },
      "outputs": [],
      "source": [
        "print(y.head())\n",
        "print(ids.head())\n",
        "print (y.value_counts()) # Check target distribution: 458 benign (2), 241 malignant (4)\n",
        "print(X.isnull().sum())# Check for missing values, 16 missing values in 'Bare_nuclei' column\n",
        "print(\"ID CHECK\")\n",
        "print(ids.isnull().sum())# Check for missing values in ids column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd5617e",
      "metadata": {
        "id": "abd5617e",
        "outputId": "2449ebed-cbad-4212-ca36-042f79fefa43"
      },
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print(\"DATA STATS\")\n",
        "print(X.info()) #all int, bare_nuclei float (683 entries non-null)\n",
        "print(X.describe()) # count, mean, std, min, 25%, 50%, 75%, max for each feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ff23089",
      "metadata": {
        "id": "0ff23089",
        "outputId": "113aeb98-2b8f-49cf-9608-ea985db0a456"
      },
      "outputs": [],
      "source": [
        "print(y.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fca17fc",
      "metadata": {
        "id": "6fca17fc"
      },
      "source": [
        "Data Cleaning - Handle missing values in 'Bare_nuclei' column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dca6649",
      "metadata": {
        "id": "6dca6649",
        "outputId": "b1cdd8bb-2b19-4e13-a5b3-668aecf7493b"
      },
      "outputs": [],
      "source": [
        "print((X.isnull().sum()/ len(X))) #2.289% missing values in 'Bare_nuclei' column\n",
        "#print(X['Bare_nuclei'].value_counts()) # Check unique values in 'Bare_nuclei' column: 1.0 402, 10.0 132\n",
        "\n",
        "# Missing values\n",
        "X_null=X.fillna(X.mean()) # Fill missing values with mean of the column, mean since it's numerical\n",
        "#X_null['Bare_nuclei'] = X_null['Bare_nuclei'].astype(int) # Convert 'Bare_nuclei' to integer type\n",
        "\n",
        "#Duplicate rows\n",
        "duplicate_check=X_null.duplicated().any() # Check for duplicate rows\n",
        "print(f\"Are there duplicate rows? {duplicate_check}\")\n",
        "print(f\"Shape before cleaning: {X_null.shape}\") # Shape before cleaning (699, 9)\n",
        "print(f\"Number of duplicate rows: {X_null.duplicated().sum()}\") # Number of duplicate rows, 237\n",
        "dup=X_null[X_null.duplicated(keep=False)]  # Display duplicate rows\n",
        "\n",
        "X_clean=X_null.drop_duplicates() # Drop duplicate rows if any\n",
        "print(f\"New shape after cleaning: {X_clean.shape}\") # New shape after cleaning (462, 9)\n",
        "y_clean=y.loc[X_clean.index] # Align target variable with cleaned features\n",
        "ids_clean=ids.loc[X_clean.index] # Align ids with cleaned features\n",
        "print(f\"Target shape after cleaning: {y_clean.shape}\") # Target shape after cleaning\n",
        "print(f\"IDs shape after cleaning: {ids_clean.shape}\") # IDs shape after cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e090cc36",
      "metadata": {
        "id": "e090cc36",
        "outputId": "8adf92a6-bd08-4dda-8d21-44176fe7ad25"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CRITICAL FIX: ENSURE TRULY UNIQUE DATA BEFORE TRAIN/TEST SPLIT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"VERIFYING DATA UNIQUENESS TO PREVENT LEAKAGE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Reset indices to avoid problems\n",
        "X_clean = X_clean.reset_index(drop=True)\n",
        "y_clean = y_clean.reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nBefore final duplicate check:\")\n",
        "print(f\"  X_clean shape: {X_clean.shape}\")\n",
        "print(f\"  y_clean shape: {y_clean.shape}\")\n",
        "\n",
        "# Check for any remaining duplicates\n",
        "remaining_duplicates = X_clean.duplicated().sum()\n",
        "print(f\"\\nRemaining duplicate rows: {remaining_duplicates}\")\n",
        "\n",
        "if remaining_duplicates > 0:\n",
        "    print(f\"  ⚠️ Found {remaining_duplicates} more duplicates - removing them...\")\n",
        "    X_clean = X_clean.drop_duplicates(keep='first').reset_index(drop=True)\n",
        "    y_clean = y_clean.loc[X_clean.index].reset_index(drop=True)\n",
        "    print(f\"  ✓ Removed. New shape: {X_clean.shape}\")\n",
        "else:\n",
        "    print(\"  ✓ No duplicates found - data is clean\")\n",
        "\n",
        "print(f\"\\nFinal unique dataset:\")\n",
        "print(f\"  Total samples: {len(X_clean)}\")\n",
        "print(f\"  Features: {X_clean.shape[1]}\")\n",
        "print(f\"  Class distribution: {y_clean.value_counts().to_dict()}\")\n",
        "\n",
        "print(\"\\n✓ Data is ready for train/test split without leakage risk\")\n",
        "print(\"=\"*80)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0129cb5",
      "metadata": {
        "id": "d0129cb5"
      },
      "source": [
        "CODE CHECK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7870694",
      "metadata": {
        "id": "a7870694",
        "outputId": "a6d92b74-ebbd-4039-ebf5-96197787438f"
      },
      "outputs": [],
      "source": [
        "print(\"DATA CLEANING CHECK\")\n",
        "print(X_clean.shape)\n",
        "print(X_clean.columns)\n",
        "print(X_clean.dtypes)\n",
        "print(X_clean.isnull().sum())\n",
        "print(y_clean.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad713e98",
      "metadata": {
        "id": "ad713e98",
        "outputId": "c9199d05-1f18-4609-b7fc-1e94f0e0c897"
      },
      "outputs": [],
      "source": [
        "# EDA\n",
        "print(X_clean.describe()) # Basic statistics after cleaning\n",
        "print(y_clean.describe()) # Target stats after cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3b558d5",
      "metadata": {
        "id": "b3b558d5"
      },
      "source": [
        "keep outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30253a5e",
      "metadata": {
        "id": "30253a5e",
        "outputId": "d89d6f11-0eb5-49de-f0b7-fa0c659233f0"
      },
      "outputs": [],
      "source": [
        "feature_list=['radius','texture', 'perimeter', 'area', 'smoothness', 'compactness', 'concavity', 'concave_points', 'symmetry', 'fractal_dimension']\n",
        "fig, axes= plt.subplots(5,2, figsize=(12,12))\n",
        "axes= axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(feature_list):\n",
        "  col_plot=[f'{feature}1', f'{feature}2', f'{feature}3']\n",
        "  X[col_plot].boxplot(ax=axes[i])\n",
        "  axes[i].set_title(f'{feature.capitalize()}- All 3 measurements', fontsize=12)\n",
        "  axes[i].set_ylabel('Value')\n",
        "  axes[i].tick_params(axis='x', rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87e594a6",
      "metadata": {
        "id": "87e594a6",
        "outputId": "456b2067-6c73-4b8f-b3f8-b78e19d7058b"
      },
      "outputs": [],
      "source": [
        "# Regression= scatter plot\n",
        "feature_list=['radius','texture', 'perimeter', 'area', 'smoothness', 'compactness', 'concavity', 'concave_points', 'symmetry', 'fractal_dimension']\n",
        "fig, axes= plt.subplots(10,3, figsize=(15,25))\n",
        "#axes=axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(feature_list):\n",
        "  for j in range(1,4):\n",
        "    column=f'{feature}{j}'\n",
        "    axes[i, j-1].scatter(range(len(X)), X[column], alpha=0.5)\n",
        "\n",
        "  if i==9:\n",
        "    axes[i, j-1].set_xlabel('Index')\n",
        "  if j==1:\n",
        "    axes[i, j-1].set_ylabel(f'{feature}')\n",
        "\n",
        "  axes[i, j-1].set_title(f'Scatter Plot of {column} -Outlier Detection')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show() #out of loop to see all graphs together\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b4a6f54",
      "metadata": {
        "id": "4b4a6f54",
        "outputId": "3268dc91-aaeb-4cd0-8cf7-6fba8c8cca24"
      },
      "outputs": [],
      "source": [
        "# Class Distribution\n",
        "print(y.value_counts(normalize=True)*100) # Class distribution percentages: B- 62.742%, M 37.258%\n",
        "plt.figure(figsize=(6,4))\n",
        "y.value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\n",
        "plt.title('Class Distribution of Target Variable')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(ticks=[0,1], labels=['Benign (2)', 'Malignant (4)'], rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2240819d",
      "metadata": {
        "id": "2240819d",
        "outputId": "8339b323-7026-4434-cf49-e54b46d02e1f"
      },
      "outputs": [],
      "source": [
        "# Correlation matrix to see relationships between features\n",
        "plt.figure(figsize=(10,8))\n",
        "correlation_matrix = X_clean.corr() #correlation matrix\n",
        "#print(correlation_matrix)\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, linecolor='white', linewidths=0.5)\n",
        "plt.title('Feature Correlation Heatmap')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc6a8ded",
      "metadata": {
        "id": "cc6a8ded",
        "outputId": "0f560ca3-7bc6-4848-a529-75ba59616058"
      },
      "outputs": [],
      "source": [
        "#Histogram for all features to find outliers\n",
        "fig, axes= plt.subplots(10,3, figsize=(18,30))\n",
        "axes= axes. flatten()\n",
        "\n",
        "for i, column in enumerate(X.columns):\n",
        "    ax=axes[i]\n",
        "    mean = X[column].mean()\n",
        "    std = X[column].std()\n",
        "    median= X[column].median()\n",
        "    x= np.linspace(X[column].min(), X[column].max(), 100)\n",
        "\n",
        "    sns.histplot(X[column], bins=30, kde=True, color='blue', edgecolor='black', ax=ax, stat= 'density', label='Histogram', line_kws={'color': 'purple', 'lw': 2, 'label': 'Actual Distribution (KDE)'})\n",
        "    ax.axvline(mean, color='red', linestyle='dashed', linewidth=2, label='Mean')\n",
        "    ax.axvline(median, color='green', linestyle='dashed', linewidth=2, label='Median')\n",
        "    ax.axvline(mean + 3*std, color='orange', linestyle='dashed', linewidth=2, label='Mean + 3*Std Dev')\n",
        "    ax.axvline(mean - 3*std, color='orange', linestyle='dashed', linewidth=2, label='Mean - 3*Std Dev')\n",
        "\n",
        "    normal_curve= stats.norm.pdf(x, mean, std)\n",
        "    ax.plot(x, normal_curve, color='red', linewidth=2, label='Normal Distribution Fit')\n",
        "    ax.set_title(f'Histogram of {column}')\n",
        "    ax.set_xlabel('Value')\n",
        "    ax.set_ylabel('Density')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "623dd829",
      "metadata": {
        "id": "623dd829"
      },
      "source": [
        "ENCODING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97dc1967",
      "metadata": {
        "id": "97dc1967",
        "outputId": "06ecb0dd-a835-402e-cc94-fce46be24740"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ENCODING - Convert M/B to 1/0\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"LABEL ENCODING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check what we're starting with\n",
        "print(f\"\\nBefore encoding:\")\n",
        "print(f\"  Unique values: {y_clean['Diagnosis'].unique()}\")\n",
        "print(f\"  Value counts:\\n{y_clean['Diagnosis'].value_counts()}\")\n",
        "\n",
        "# CORRECT encoding for WDBC dataset (M/B not 2/4!)\n",
        "y_encoded = y_clean['Diagnosis'].replace({'M': 1, 'B': 0})  # ← CHANGED!\n",
        "\n",
        "# Verify encoding worked\n",
        "print(f\"\\nAfter encoding:\")\n",
        "print(f\"  Type: {type(y_encoded)}\")\n",
        "print(f\"  Shape: {y_encoded.shape}\")\n",
        "print(f\"  Unique values: {y_encoded.unique()}\")\n",
        "print(f\"  Value counts:\\n{y_encoded.value_counts()}\")\n",
        "print(f\"\\nMapping:\")\n",
        "print(f\"  M (Malignant) → 1\")\n",
        "print(f\"  B (Benign)    → 0\")\n",
        "\n",
        "# Verify it's numeric now\n",
        "if y_encoded.dtype == 'object':\n",
        "    print(\"\\n❌ ERROR: Labels are still strings!\")\n",
        "else:\n",
        "    print(f\"\\n✓ Labels successfully encoded to {y_encoded.dtype}\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5162388d",
      "metadata": {
        "id": "5162388d"
      },
      "source": [
        "TRAIN-TEST SPLIT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1092d8af",
      "metadata": {
        "id": "1092d8af"
      },
      "source": [
        "References:\n",
        "https://www.datacamp.com/tutorial/decision-tree-classification-python\n",
        "https://medium.com/@enozeren/building-a-decision-tree-from-scratch-324b9a5ed836 from pg194"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff7b512a",
      "metadata": {
        "id": "ff7b512a",
        "outputId": "4e7c66d2-a47d-43ee-a91d-9c79f5dca2da"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TRAIN/TEST SPLIT - FIXED VERSION (No sklearn, no data leakage)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TRAIN/TEST SPLIT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# CRITICAL: Reset indices to ensure they match\n",
        "X_clean = X_clean.reset_index(drop=True)\n",
        "y_encoded = y_encoded.reset_index(drop=True)\n",
        "\n",
        "print(f\"Data shapes before split:\")\n",
        "print(f\"  X_clean: {X_clean.shape}\")\n",
        "print(f\"  y_encoded: {y_encoded.shape}\")\n",
        "\n",
        "# Convert to numpy arrays first to avoid index issues\n",
        "X_array = X_clean.values\n",
        "y_array = y_encoded.values.ravel()\n",
        "\n",
        "# Get indices for each class\n",
        "benign_mask = (y_array == 0)\n",
        "malignant_mask = (y_array == 1)\n",
        "\n",
        "benign_indices = np.where(benign_mask)[0]  # Use np.where to get actual positions\n",
        "malignant_indices = np.where(malignant_mask)[0]\n",
        "\n",
        "print(f\"\\nClass counts:\")\n",
        "print(f\"  Benign (0): {len(benign_indices)}\")\n",
        "print(f\"  Malignant (1): {len(malignant_indices)}\")\n",
        "\n",
        "# Shuffle indices within each class\n",
        "np.random.shuffle(benign_indices)\n",
        "np.random.shuffle(malignant_indices)\n",
        "\n",
        "# Split indices (80-20 split)\n",
        "test_size = 0.2\n",
        "benign_split = int(len(benign_indices) * (1 - test_size))\n",
        "malignant_split = int(len(malignant_indices) * (1 - test_size))\n",
        "\n",
        "benign_train_idx = benign_indices[:benign_split]\n",
        "benign_test_idx = benign_indices[benign_split:]\n",
        "\n",
        "malignant_train_idx = malignant_indices[:malignant_split]\n",
        "malignant_test_idx = malignant_indices[malignant_split:]\n",
        "\n",
        "# Merge train and test indices\n",
        "train_idx = np.concatenate([benign_train_idx, malignant_train_idx])\n",
        "test_idx = np.concatenate([benign_test_idx, malignant_test_idx])\n",
        "\n",
        "# Shuffle merged sets\n",
        "np.random.shuffle(train_idx)\n",
        "np.random.shuffle(test_idx)\n",
        "\n",
        "# Use iloc (position-based) instead of loc (label-based)\n",
        "X_train = X_clean.iloc[train_idx].reset_index(drop=True)\n",
        "X_test = X_clean.iloc[test_idx].reset_index(drop=True)\n",
        "y_train = y_encoded.iloc[train_idx].reset_index(drop=True)\n",
        "y_test = y_encoded.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nTrain set:\")\n",
        "print(f\"  X_train: {X_train.shape}\")\n",
        "print(f\"  y_train: {y_train.shape}\")\n",
        "print(f\"  Class 0: {np.sum(y_train == 0)}, Class 1: {np.sum(y_train == 1)}\")\n",
        "\n",
        "print(f\"\\nTest set:\")\n",
        "print(f\"  X_test: {X_test.shape}\")\n",
        "print(f\"  y_test: {y_test.shape}\")\n",
        "print(f\"  Class 0: {np.sum(y_test == 0)}, Class 1: {np.sum(y_test == 1)}\")\n",
        "\n",
        "# Verify no overlap\n",
        "print(f\"\\n✓ Verifying no index overlap...\")\n",
        "train_set = set(train_idx)\n",
        "test_set = set(test_idx)\n",
        "overlap = train_set.intersection(test_set)\n",
        "print(f\"  Overlapping indices: {len(overlap)}\")\n",
        "\n",
        "if len(overlap) == 0:\n",
        "    print(\"  ✓ Perfect! No index overlap!\")\n",
        "else:\n",
        "    print(f\"  ✗ ERROR: {len(overlap)} overlapping indices!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8abdc073",
      "metadata": {
        "id": "8abdc073",
        "outputId": "363bc4b1-19d6-48d9-a38c-354faa7cbb0b"
      },
      "outputs": [],
      "source": [
        "#CODE CHECK\n",
        "print(\"X set\", X_train.shape, X_test.shape)\n",
        "print(\"Y set\", y_train.shape, y_test.shape)\n",
        "print(\"Training distribution\",y_train.value_counts(), y_train.value_counts(normalize=True)*100)\n",
        "print(\"Testing distribution\",y_test.value_counts(), y_test.value_counts(normalize=True)*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05c82c94",
      "metadata": {
        "id": "05c82c94",
        "outputId": "1704cac2-2a4a-4a31-f16d-94be6a33f01d"
      },
      "outputs": [],
      "source": [
        "# SCALING\n",
        "train_mean= X_train.mean()\n",
        "train_std= X_train.std()\n",
        "print(\"Train mean\", train_mean)\n",
        "print(\"Train std\", train_std)\n",
        "\n",
        "X_train_scaled= (X_train - train_mean)/ train_std\n",
        "X_test_scaled= (X_test - train_mean)/ train_std # standarised using train mean and std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5581b9a7",
      "metadata": {
        "id": "5581b9a7"
      },
      "outputs": [],
      "source": [
        "# Convert scaled pandas DataFrames to NumPy arrays\n",
        "X_train_np = X_train_scaled.values\n",
        "X_test_np  = X_test_scaled.values\n",
        "\n",
        "y_train_np = y_train.values.ravel()\n",
        "y_test_np  = y_test.values.ravel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e55dfb7b",
      "metadata": {
        "id": "e55dfb7b",
        "outputId": "8da2e044-f018-4321-84a4-b452e10432a3"
      },
      "outputs": [],
      "source": [
        "print(\"Scaled training set\", X_train_scaled.mean())\n",
        "print(\"Scaled testing set\", X_test_scaled.mean())\n",
        "print(\"Scaled training set std\", X_train_scaled.std())  #all std=1\n",
        "print(\"Scaled testing set std\", X_test_scaled.std())  #close to 1\n",
        "print(\"Shape\", X_train_scaled.shape, X_test_scaled.shape) #(738,9) (186,9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b36ef56",
      "metadata": {
        "id": "8b36ef56",
        "outputId": "caec4950-3931-4e8a-db93-5d4085f29b6c"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"FINAL DIAGNOSTIC CHECK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Check data shapes\n",
        "print(\"\\n1. DATA SHAPES:\")\n",
        "print(f\"   X_train_np: {X_train_np.shape}\")\n",
        "print(f\"   X_test_np:  {X_test_np.shape}\")\n",
        "print(f\"   y_train_np: {y_train_np.shape}\")\n",
        "print(f\"   y_test_np:  {y_test_np.shape}\")\n",
        "\n",
        "# 2. Check for data overlap\n",
        "print(\"\\n2. CHECKING FOR DATA LEAKAGE:\")\n",
        "train_set = set(map(tuple, X_train_np))\n",
        "test_set = set(map(tuple, X_test_np))\n",
        "overlap = train_set.intersection(test_set)\n",
        "print(f\"   Overlapping samples: {len(overlap)}\")\n",
        "if len(overlap) > 0:\n",
        "    print(\"   ⚠️ DATA LEAKAGE DETECTED!\")\n",
        "else:\n",
        "    print(\"   ✓ No overlap - train/test properly separated\")\n",
        "\n",
        "# 3. Check label distributions\n",
        "print(\"\\n3. LABEL DISTRIBUTIONS:\")\n",
        "print(f\"   Train: {np.bincount(y_train_np)} (class 0, class 1)\")\n",
        "print(f\"   Test:  {np.bincount(y_test_np)} (class 0, class 1)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f663f85d",
      "metadata": {
        "id": "f663f85d"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41fcc317",
      "metadata": {
        "id": "41fcc317"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    \"\"\"\n",
        "    Logistic regression with gradient descent.\n",
        "\n",
        "    Model: P(y=1|x) = σ(w^T x + b) where σ is the sigmoid function\n",
        "    Loss: Cross-entropy with L2 regularization\n",
        "\n",
        "    We use batch gradient descent to optimize. Following Chapter 4 of Hastie et al.\n",
        "    (Elements of Statistical Learning, 2009) for the theory, with gradient formulas\n",
        "    from Bishop (2006).\n",
        "\n",
        "    Parameters:\n",
        "        learning_rate: Step size α for gradient updates\n",
        "        num_iterations: How many passes through the data\n",
        "        regularization: L2 penalty λ to prevent overfitting\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=1000, regularization=0.01):\n",
        "\n",
        "        \"\"\"Initialize logistic regression parameters\"\"\"\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.regularization = regularization\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.losses = []  # Track convergence\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Sigmoid function: σ(z) = 1 / (1 + e^(-z))\n",
        "        \"\"\"\n",
        "        #Squashes any input to a probability between 0 and 1\n",
        "\n",
        "        # Clip to prevent overflow in exp()\n",
        "        z = np.clip(z, -500, 500)\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train using gradient descent.\n",
        "\n",
        "        At each iteration:\n",
        "            1. Compute predictions: ŷ = σ(Xw + b)\n",
        "            2. Calculate gradients: ∂L/∂w = (1/n)X^T(ŷ - y) + λw\n",
        "            3. Update: w ← w - α(∂L/∂w)\n",
        "\n",
        "        Gradients derived from cross-entropy loss\n",
        "        \"\"\"\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize parameters to zeros\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Gradient descent optimization\n",
        "        for iteration in range(self.num_iterations):\n",
        "            # Forward pass: compute predictions\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_predicted = self._sigmoid(linear_model)\n",
        "\n",
        "            # Compute gradients (using calculus chain rule)\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            # Add L2 regularization gradient\n",
        "            dw += (self.regularization / n_samples) * self.weights\n",
        "\n",
        "            # Update parameters (gradient descent step)\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "            # Track loss for convergence monitoring\n",
        "            if iteration % 100 == 0:\n",
        "                loss = self._compute_loss(X, y)\n",
        "                self.losses.append(loss)\n",
        "\n",
        "    def _compute_loss(self, X, y):\n",
        "        \"\"\"\n",
        "        Compute cross-entropy loss with L2 regularization.\n",
        "\n",
        "        Loss Function Components:\n",
        "            1. Cross-Entropy: Measures prediction error\n",
        "               -1/n Σ[y*log(ŷ) + (1-y)*log(1-ŷ)]\n",
        "\n",
        "            2. L2 Regularization: Prevents overfitting\n",
        "               λ/(2n) ||w||²\n",
        "\n",
        "        Derivation:\n",
        "            From maximum likelihood estimation (MLE) of Bernoulli distribution\n",
        "            Negative log-likelihood = cross-entropy\n",
        "\n",
        "        Args:\n",
        "            X: Features\n",
        "            y: True labels\n",
        "\n",
        "        Returns:\n",
        "            Total loss value\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self._sigmoid(linear_model)\n",
        "\n",
        "        # Cross-entropy loss (avoid log(0) with epsilon)\n",
        "        epsilon = 1e-9\n",
        "        cross_entropy = -np.mean(\n",
        "            y * np.log(y_predicted + epsilon) +\n",
        "            (1 - y) * np.log(1 - y_predicted + epsilon)\n",
        "        )\n",
        "\n",
        "        # L2 regularization penalty\n",
        "        l2_penalty = (self.regularization / (2 * n_samples)) * np.sum(self.weights ** 2)\n",
        "\n",
        "        return cross_entropy + l2_penalty\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Return probability of positive class using sigmoid(w·x + b).\"\"\"\n",
        "\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        return self._sigmoid(linear_model)\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        \"\"\"Predict class labels: 1 if P(y=1) ≥ threshold, else 0.\"\"\"\n",
        "        probabilities = self.predict_proba(X)\n",
        "        return (probabilities >= threshold).astype(int)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd721b44",
      "metadata": {
        "id": "bd721b44"
      },
      "source": [
        "## Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f715021",
      "metadata": {
        "id": "5f715021"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "COMPLETELY REWRITTEN SVM - DEBUGGED VERSION\n",
        "============================================\n",
        "\n",
        "This version includes:\n",
        "1. Proper gradient computation (fixed the bug)\n",
        "2. Detailed debugging output\n",
        "3. Learning rate scheduling (helps convergence)\n",
        "4. Verification that it's learning both classes\n",
        "\n",
        "There's no \"kernel\" vs \"regular\" SVM distinction we need to make.\n",
        "We're implementing a LINEAR SVM (the simplest kind).\n",
        "Kernel SVM is much more complex and not required.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class SupportVectorMachine:\n",
        "    \"\"\"\n",
        "    Linear SVM with hinge loss optimization using Pegasos algorithm.\n",
        "\n",
        "    This is a LINEAR SVM (no kernel). Kernel SVM would require:\n",
        "    - Kernel functions (RBF, polynomial, etc.)\n",
        "    - Dual formulation\n",
        "    - Much more complex optimization\n",
        "\n",
        "    We're implementing the PRIMAL formulation with sub-gradient descent,\n",
        "    which is simpler and works well for linearly separable data.\n",
        "\n",
        "    Based on:\n",
        "    - Cortes & Vapnik (1995) - Original SVM paper\n",
        "    - Shalev-Shwartz et al. (2011) - Pegasos algorithm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=0.001, lambda_param=0.01, num_iterations=1000, verbose=False):\n",
        "        \"\"\"\n",
        "        Initialize SVM hyperparameters.\n",
        "\n",
        "        Args:\n",
        "            learning_rate: Initial step size for gradient descent\n",
        "            lambda_param: Regularization parameter (C = 1/lambda in some formulations)\n",
        "            num_iterations: Number of passes through the data\n",
        "            verbose: If True, print debugging info during training\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lambda_param = lambda_param\n",
        "        self.num_iterations = num_iterations\n",
        "        self.verbose = verbose\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.loss_history = []  # Track loss over time\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train SVM using Pegasos (Primal Estimated sub-GrAdient SOlver for SVM).\n",
        "\n",
        "        The algorithm:\n",
        "        1. Convert labels to {-1, +1}\n",
        "        2. Initialize weights to zero\n",
        "        3. For each iteration:\n",
        "           - For each sample:\n",
        "             - Check if correctly classified with margin\n",
        "             - Update weights accordingly\n",
        "\n",
        "        Key insight: We want to find w, b such that:\n",
        "            y_i(w·x_i + b) ≥ 1 for all i (correct with margin)\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Convert labels from {0, 1} to {-1, +1}\n",
        "        # This is standard for SVM: allows using y(w·x+b) for margin\n",
        "        y_svm = np.where(y <= 0, -1, 1)\n",
        "\n",
        "        # Verify label conversion\n",
        "        if self.verbose:\n",
        "            print(f\"\\nLabel conversion:\")\n",
        "            print(f\"  Original unique: {np.unique(y)}\")\n",
        "            print(f\"  Converted unique: {np.unique(y_svm)}\")\n",
        "            print(f\"  Distribution: {np.bincount(y_svm + 1)}\")  # Shift to [0,2] for bincount\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Track how many times we update for each class\n",
        "        updates_class_neg = 0\n",
        "        updates_class_pos = 0\n",
        "\n",
        "        # Sub-gradient descent\n",
        "        for iteration in range(self.num_iterations):\n",
        "            # Optional: learning rate decay (helps convergence)\n",
        "            # current_lr = self.learning_rate / (1 + iteration / 100)\n",
        "            current_lr = self.learning_rate  # Or use fixed learning rate\n",
        "\n",
        "            total_loss = 0\n",
        "            num_violations = 0\n",
        "\n",
        "            for idx in range(n_samples):\n",
        "                x_i = X[idx]\n",
        "                y_i = y_svm[idx]\n",
        "\n",
        "                # Compute margin: y_i(w·x_i + b)\n",
        "                margin = y_i * (np.dot(self.weights, x_i) + self.bias)\n",
        "\n",
        "                # Hinge loss: max(0, 1 - margin)\n",
        "                loss = max(0, 1 - margin)\n",
        "                total_loss += loss\n",
        "\n",
        "                if margin >= 1:\n",
        "                    # Correctly classified with margin\n",
        "                    # Only apply regularization: w ← w - α(λw)\n",
        "                    self.weights -= current_lr * (self.lambda_param * self.weights)\n",
        "                else:\n",
        "                    # Violated margin (misclassified or too close)\n",
        "                    # Apply full gradient: w ← w - α(λw - y_i*x_i)\n",
        "                    # CRITICAL: Must be y_i * x_i (vector), NOT np.dot(x_i, y_i) (scalar)\n",
        "                    self.weights -= current_lr * (\n",
        "                        self.lambda_param * self.weights - y_i * x_i\n",
        "                    )\n",
        "                    self.bias -= current_lr * (-y_i)  # Gradient of bias\n",
        "\n",
        "                    num_violations += 1\n",
        "                    if y_i == -1:\n",
        "                        updates_class_neg += 1\n",
        "                    else:\n",
        "                        updates_class_pos += 1\n",
        "\n",
        "            # Track average loss\n",
        "            avg_loss = total_loss / n_samples\n",
        "            self.loss_history.append(avg_loss)\n",
        "\n",
        "            # Print progress\n",
        "            if self.verbose and (iteration % 200 == 0 or iteration == self.num_iterations - 1):\n",
        "                print(f\"Iteration {iteration:4d}: Loss = {avg_loss:.4f}, \"\n",
        "                      f\"Violations = {num_violations}/{n_samples}\")\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"\\nTraining complete!\")\n",
        "            print(f\"Final weights range: [{self.weights.min():.4f}, {self.weights.max():.4f}]\")\n",
        "            print(f\"Final bias: {self.bias:.4f}\")\n",
        "            print(f\"Updates for class -1: {updates_class_neg}\")\n",
        "            print(f\"Updates for class +1: {updates_class_pos}\")\n",
        "\n",
        "            # Check if both classes were learned\n",
        "            if updates_class_neg == 0 or updates_class_pos == 0:\n",
        "                print(\"⚠ WARNING: No updates for one class! Model may be degenerate.\")\n",
        "\n",
        "    def _decision_function(self, X):\n",
        "        \"\"\"\n",
        "        Compute decision values: w·x + b\n",
        "\n",
        "        Interpretation:\n",
        "        - Positive: Classified as +1 (class 1)\n",
        "        - Negative: Classified as -1 (class 0)\n",
        "        - Magnitude: Confidence (distance from decision boundary)\n",
        "        \"\"\"\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels.\n",
        "\n",
        "        Decision rule: sign(w·x + b)\n",
        "        - If w·x + b ≥ 0 → predict 1\n",
        "        - If w·x + b < 0  → predict 0\n",
        "        \"\"\"\n",
        "        decision = self._decision_function(X)\n",
        "        return np.where(decision >= 0, 1, 0)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Estimate probabilities using sigmoid of decision function.\n",
        "\n",
        "        Note: SVMs don't naturally produce probabilities!\n",
        "        This is a rough approximation. For better calibration, use Platt scaling.\n",
        "        \"\"\"\n",
        "        decision = self._decision_function(X)\n",
        "        # Clip to prevent overflow\n",
        "        decision_clipped = np.clip(decision, -500, 500)\n",
        "        return 1 / (1 + np.exp(-decision_clipped))\n",
        "\n",
        "    def get_decision_stats(self, X, y):\n",
        "        \"\"\"\n",
        "        Debugging method: Check decision function values and predictions.\n",
        "        \"\"\"\n",
        "        decisions = self._decision_function(X)\n",
        "        predictions = self.predict(X)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"DECISION FUNCTION STATISTICS\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Decision values range: [{decisions.min():.4f}, {decisions.max():.4f}]\")\n",
        "        print(f\"Decision values mean: {decisions.mean():.4f}\")\n",
        "        print(f\"Decision values std: {decisions.std():.4f}\")\n",
        "\n",
        "        print(f\"\\nPredictions unique: {np.unique(predictions)}\")\n",
        "        print(f\"Predictions distribution: {np.bincount(predictions)}\")\n",
        "\n",
        "        print(f\"\\nTrue labels unique: {np.unique(y)}\")\n",
        "        print(f\"True labels distribution: {np.bincount(y)}\")\n",
        "\n",
        "        # Check decision values by class\n",
        "        for cls in [0, 1]:\n",
        "            mask = y == cls\n",
        "            if np.sum(mask) > 0:\n",
        "                cls_decisions = decisions[mask]\n",
        "                print(f\"\\nClass {cls} decision values:\")\n",
        "                print(f\"  Range: [{cls_decisions.min():.4f}, {cls_decisions.max():.4f}]\")\n",
        "                print(f\"  Mean: {cls_decisions.mean():.4f}\")\n",
        "\n",
        "        # Check if model is just predicting one class\n",
        "        if len(np.unique(predictions)) == 1:\n",
        "            print(\"\\n⚠ WARNING: Model is predicting only ONE class!\")\n",
        "            print(\"   Possible causes:\")\n",
        "            print(\"   1. Features not scaled properly\")\n",
        "            print(\"   2. Learning rate too high/low\")\n",
        "            print(\"   3. Lambda too high\")\n",
        "            print(\"   4. Not enough iterations\")\n",
        "        else:\n",
        "            print(\"\\n✓ Model is predicting both classes\")\n",
        "\n",
        "        print(\"=\"*60)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29e6cde2",
      "metadata": {
        "id": "29e6cde2"
      },
      "source": [
        "## K-Nearest Neighbor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b26ec8ba",
      "metadata": {
        "id": "b26ec8ba"
      },
      "outputs": [],
      "source": [
        "class KNearestNeighbors:\n",
        "    \"\"\"\n",
        "    KNN classifier.\n",
        "\n",
        "    Instead of training a model, it just stores the training data and makes predictions by finding the k closest examples.\n",
        "    Classification is by majority vote.\n",
        "\n",
        "    We support Euclidean and Manhattan distance metrics.\n",
        "\n",
        "    Parameters:\n",
        "        k: Number of neighbors to consider\n",
        "        distance_metric: 'euclidean' or 'manhattan'\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, k=5, distance_metric='euclidean'):\n",
        "        \"\"\"Set k and distance metric.\"\"\"\n",
        "        self.k = k\n",
        "        self.distance_metric = distance_metric\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Just store the training data. KNN doesn't actually \"train\" anything.\n",
        "        All the work happens during prediction.\n",
        "        \"\"\"\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def _compute_distance(self, x1, x2):\n",
        "        \"\"\"\n",
        "        Calculate distance between two points.\n",
        "\n",
        "        Euclidean: d(x,y) = √(Σᵢ(xᵢ - yᵢ)²) - straight line distance\n",
        "        Manhattan: d(x,y) = Σᵢ|xᵢ - yᵢ| - grid/taxicab distance\n",
        "        \"\"\"\n",
        "        if self.distance_metric == 'euclidean':\n",
        "            return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "        elif self.distance_metric == 'manhattan':\n",
        "            return np.sum(np.abs(x1 - x2))\n",
        "        else:\n",
        "            raise ValueError(\"Distance metric must be 'euclidean' or 'manhattan'\")\n",
        "\n",
        "    def _get_neighbors(self, x):\n",
        "        \"\"\"\n",
        "        Find KNN by computing all distances and taking the k smallest.\n",
        "\n",
        "        This is a naive O(n) search.\n",
        "        \"\"\"\n",
        "        # Calculate distances to all training samples\n",
        "        distances = [self._compute_distance(x, x_train)\n",
        "                    for x_train in self.X_train]\n",
        "\n",
        "        # Get indices of k smallest distances\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "\n",
        "        return k_indices\n",
        "\n",
        "    def _majority_vote(self, neighbor_labels):\n",
        "        \"\"\"\n",
        "        Find the most common class among the k neighbors. Simple majority wins.\n",
        "\n",
        "        We considered distance-weighted voting but it didn't improve results\n",
        "        on our dataset, so stuck with the simpler approach.\n",
        "        \"\"\"\n",
        "        unique_labels, counts = np.unique(neighbor_labels, return_counts=True)\n",
        "        max_count_idx = np.argmax(counts)\n",
        "        return unique_labels[max_count_idx]\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict by finding k nearest neighbors for each sample and taking majority vote.\"\"\"\n",
        "        predictions = []\n",
        "\n",
        "        for x in X:\n",
        "            # Find k nearest neighbors\n",
        "            k_indices = self._get_neighbors(x)\n",
        "\n",
        "            # Get labels of neighbors\n",
        "            k_nearest_labels = self.y_train[k_indices]\n",
        "\n",
        "            # Majority vote\n",
        "            prediction = self._majority_vote(k_nearest_labels)\n",
        "            predictions.append(prediction)\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Estimate P(y=1) as the proportion of positive neighbors: (# positive) / k\n",
        "        Simple frequency-based probability.\n",
        "        \"\"\"\n",
        "        probabilities = []\n",
        "\n",
        "        for x in X:\n",
        "            k_indices = self._get_neighbors(x)\n",
        "            k_nearest_labels = self.y_train[k_indices]\n",
        "\n",
        "            # Probability = proportion of positive class\n",
        "            prob_positive = np.sum(k_nearest_labels == 1) / self.k\n",
        "            probabilities.append(prob_positive)\n",
        "\n",
        "        return np.array(probabilities)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "267ba562",
      "metadata": {
        "id": "267ba562"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c00bf321",
      "metadata": {
        "id": "c00bf321"
      },
      "source": [
        "Reference:\n",
        "https://github.com/shivamms/books/blob/master/nlp/Data%20Science%20from%20Scratch-%20First%20Principles%20with%20Python.pdf pg204"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c9fcf72",
      "metadata": {
        "id": "3c9fcf72"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None): #all set to none\n",
        "        self.feature = feature          # Feature index for splitting\n",
        "        self.threshold = threshold      # Threshold value for splitting\n",
        "        self.left = left                # Left child node\n",
        "        self.right = right              # Right child node\n",
        "        self.value = value              # Class label for leaf nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d886284e",
      "metadata": {
        "id": "d886284e"
      },
      "outputs": [],
      "source": [
        "class DecisionTree:\n",
        "\n",
        "    \"\"\"\n",
        "    Decision tree classifier using information gain for splits.\n",
        "\n",
        "    We're implementing CART (Breiman et al., 1984) with entropy-based splitting\n",
        "    following Quinlan's approach (1986). The tree grows recursively, choosing the\n",
        "    best split at each node by maximizing information gain.\n",
        "\n",
        "    Parameters:\n",
        "        max_depth: How deep the tree can grow (prevents overfitting)\n",
        "        min_samples_split: Min samples needed to split a node\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_depth=10, min_samples_split=2): #default values set to 10 questions\n",
        "        \"\"\"Initialize the tree with stopping criteria to prevent overfitting.\"\"\"\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Build the tree by recursively finding the best splits.\"\"\"\n",
        "        self.root = self._grow_tree(X, y) #tree starts growing from root\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        \"\"\"\n",
        "        Recursively grow the tree. Stops when we hit max depth, run out of samples,\n",
        "        or all samples belong to the same class.\n",
        "        \"\"\"\n",
        "        A_samples, A_features = X.shape\n",
        "        unique_classes = np.unique(y)\n",
        "\n",
        "        # Stopping criteria\n",
        "        if (len(unique_classes) == 1 or\n",
        "            A_samples < self.min_samples_split or\n",
        "            depth >= self.max_depth): #max set at 10\n",
        "            leaf_value = self._mode_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        # Find the best split\n",
        "        best_feature, best_threshold = self._best_split(X, y, A_features)\n",
        "\n",
        "        # If no valid split is found, create a leaf node\n",
        "        if best_feature is None:\n",
        "            leaf_value = self._mode_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        # Split the dataset\n",
        "        left_indices = X[:, best_feature] < best_threshold\n",
        "        right_indices = X[:, best_feature] >= best_threshold\n",
        "        left_subtree = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_subtree = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "        return Node(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
        "\n",
        "    def _best_split(self, X, y, A_features): #best feature and threshold to split on\n",
        "        \"\"\"\n",
        "        Find the best feature and threshold to split on.\n",
        "\n",
        "        We just try all features and all possible thresholds (greedy approach\n",
        "        from Quinlan 1986) and pick whichever gives the highest information gain.\n",
        "        \"\"\"\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "\n",
        "        for feature_index in range(A_features):\n",
        "            X_column=X[:, feature_index] #store feature column\n",
        "            thresholds = np.unique(X_column) #use stored column to get unique thresholds\n",
        "            for threshold in thresholds:\n",
        "                gain = self._information_gain(y, X_column, threshold)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feature_index\n",
        "                    split_threshold = threshold\n",
        "\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "    #Information gain entropy\n",
        "    def _information_gain(self, y, X_column, threshold):\n",
        "        \"\"\"\n",
        "        Calculate information gain from a split.\n",
        "\n",
        "        Formula: IG = H(parent) - weighted_average(H(children))\n",
        "\n",
        "        This measures how much splitting reduces uncertainty. Higher gain = better split.\n",
        "        Based on Shannon entropy (1948), applied to decision trees by Quinlan (1986).\n",
        "        \"\"\"\n",
        "        parent_entropy = self._entropy(y)\n",
        "        # Generate split\n",
        "        left_indices = X_column < threshold\n",
        "        right_indices = X_column >= threshold\n",
        "        if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:\n",
        "            return 0\n",
        "\n",
        "        #weighted avg of child entropies\n",
        "        n = len(y)\n",
        "        n_left, n_right = len(y[left_indices]), len(y[right_indices])\n",
        "        e_left, e_right = self._entropy(y[left_indices]), self._entropy(y[right_indices])\n",
        "        child_entropy = (n_left / n) * e_left + (n_right / n) * e_right\n",
        "        #info gain is parent entropy - child entropy\n",
        "        info_gain= parent_entropy - child_entropy\n",
        "        return info_gain\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        \"\"\"\n",
        "        Calculate entropy: H(S) = -Σ(p_i * log2(p_i))\n",
        "\n",
        "        Measures impurity/uncertainty in the data. Pure node (all same class) = 0 entropy.\n",
        "        From Shannon (1948).\n",
        "        \"\"\"\n",
        "        class_labels, counts = np.unique(y, return_counts=True)\n",
        "        probabilities = counts / len(y)\n",
        "        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-9)) #add small value to avoid log(0)\n",
        "        return entropy\n",
        "\n",
        "    def _mode_label(self, y):\n",
        "        \"\"\"Return the most frequent class label (for leaf nodes).\"\"\"\n",
        "        values, counts = np.unique(y, return_counts=True)\n",
        "        max_count_value= np.argmax(counts) #index of max count\n",
        "        most_common = values[max_count_value]\n",
        "        return most_common\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict class labels by traversing the tree for each sample.\"\"\"\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        \"\"\"Recursively follow the decision rules until we hit a leaf.\"\"\"\n",
        "        if node.value is not None: #yes=leaf, no= decision node\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        else:\n",
        "            return self._traverse_tree(x, node.right)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3e0ac90",
      "metadata": {
        "id": "f3e0ac90"
      },
      "source": [
        "## Evaluation Metric Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81d972a1",
      "metadata": {
        "id": "81d972a1",
        "outputId": "f69cf7bb-7fc0-4fad-836c-732c4d7bf712"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(y_true, y_pred, model_name=None, verbose=True):\n",
        "    \"\"\"\n",
        "    Calculate standard classification metrics from scratch.\n",
        "\n",
        "    Metrics:\n",
        "        - Accuracy: (TP + TN) / Total\n",
        "        - Precision: TP / (TP + FP)\n",
        "        - Recall: TP / (TP + FN)\n",
        "        - F1: 2 * (Precision * Recall) / (Precision + Recall)\n",
        "\n",
        "    We compute these manually to show understanding of the evaluation process.\n",
        "\n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred: Predicted labels\n",
        "        model_name: Optional name for printing\n",
        "        verbose: If True, print the metrics (default: True)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with all metrics and confusion matrix values\n",
        "    \"\"\"\n",
        "    # Calculate confusion matrix components\n",
        "    true_positive = int(np.sum((y_true == 1) & (y_pred == 1)))\n",
        "    true_negative = int(np.sum((y_true == 0) & (y_pred == 0)))\n",
        "    false_positive = int(np.sum((y_true == 0) & (y_pred == 1)))\n",
        "    false_negative = int(np.sum((y_true == 1) & (y_pred == 0)))\n",
        "\n",
        "    # Calculate metrics\n",
        "    total = len(y_true)\n",
        "    accuracy = (true_positive + true_negative) / total\n",
        "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
        "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    # Print results only if verbose=True\n",
        "    if verbose:\n",
        "        print(f\"\\nEVALUATION RESULTS: {model_name if model_name else 'Model'}\")\n",
        "        print(f\"\\nPerformance Metrics:\")\n",
        "        print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:6.2f}%)\")\n",
        "        print(f\"  Precision: {precision:.4f} ({precision*100:6.2f}%)\")\n",
        "        print(f\"  Recall:    {recall:.4f} ({recall*100:6.2f}%)\")\n",
        "        print(f\"  F1-Score:  {f1_score:.4f} ({f1_score*100:6.2f}%)\")\n",
        "\n",
        "        print(f\"\\nConfusion Matrix:\")\n",
        "        print(f\"                Predicted\")\n",
        "        print(f\"                0       1\")\n",
        "        print(f\"  Actual  0    {true_negative:3d}    {false_positive:3d}   (TN)  (FP)\")\n",
        "        print(f\"          1    {false_negative:3d}    {true_positive:3d}   (FN)  (TP)\")\n",
        "\n",
        "    return {\n",
        "        \"Model\": model_name if model_name else \"Model\",\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1_score,\n",
        "        \"tn\": true_negative,\n",
        "        \"fp\": false_positive,\n",
        "        \"fn\": false_negative,\n",
        "        \"tp\": true_positive\n",
        "    }\n",
        "\n",
        "print(\"Metrics function defined successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "779769f9",
      "metadata": {
        "id": "779769f9"
      },
      "source": [
        "## Hyperparameter Tuning - DT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68ec0a74",
      "metadata": {
        "id": "68ec0a74"
      },
      "outputs": [],
      "source": [
        "def grid_search_dt(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Try different combinations of hyperparameters to find the best one for Decision Tree.\n",
        "    It just brute force through all combinations.\n",
        "    \"\"\"\n",
        "    best_acc = 0\n",
        "    best_param = {}\n",
        "    results = []\n",
        "\n",
        "    max_depth = [2, 4, 6, 8, 10, 15, 20, 25, 30]  # different tree depths to try\n",
        "    min_samples_split = [2, 5, 10, 15, 20]  # minimum samples needed to split\n",
        "\n",
        "    total_combo = len(max_depth) * len(min_samples_split)\n",
        "    print(f\"\\nTesting {total_combo} parameter combinations for Decision Tree...\")\n",
        "\n",
        "    combo_count = 0\n",
        "    for depth in max_depth:\n",
        "        for min_samples in min_samples_split:\n",
        "            combo_count += 1\n",
        "            print(f\"[{combo_count}/{total_combo}] Testing: max_depth={depth}, min_samples_split={min_samples}\", end='')\n",
        "\n",
        "            dtree = DecisionTree(max_depth=depth, min_samples_split=min_samples)\n",
        "            dtree.fit(X_train, y_train)\n",
        "            y_pred = dtree.predict(X_test)\n",
        "            metrics = calculate_metrics(y_test, y_pred, verbose=False )\n",
        "\n",
        "            results.append({\n",
        "                'max_depth': depth,\n",
        "                'min_samples_split': min_samples,\n",
        "                'accuracy': metrics['accuracy'],\n",
        "                'precision': metrics['precision'],\n",
        "                'recall': metrics['recall'],\n",
        "                'f1_score': metrics['f1_score']\n",
        "            })\n",
        "\n",
        "            if metrics['accuracy'] > best_acc:\n",
        "                best_acc = metrics['accuracy']\n",
        "                best_param = {'max_depth': depth, 'min_samples_split': min_samples}\n",
        "                print(f\" -> New best! Accuracy: {best_acc*100:.2f}%\")\n",
        "            else:\n",
        "                print(f\" -> Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
        "\n",
        "    print(f\"\\nBest parameters found: {best_param}\")\n",
        "    print(f\"Best accuracy: {best_acc*100:.2f}%\")\n",
        "\n",
        "    return best_param, results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dc49816",
      "metadata": {
        "id": "7dc49816"
      },
      "source": [
        "## Hyperparameter Tuning - KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc2f01d",
      "metadata": {
        "id": "2fc2f01d"
      },
      "outputs": [],
      "source": [
        "def grid_search_knn(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Grid search for KNN to find optimal k and distance metric.\n",
        "    \"\"\"\n",
        "    best_acc = 0\n",
        "    best_param = {}\n",
        "    results = []\n",
        "\n",
        "    k_values = [1, 3, 5, 7, 9, 11, 15, 19]\n",
        "    distance_metrics = ['euclidean', 'manhattan']\n",
        "\n",
        "    total_combo = len(k_values) * len(distance_metrics)\n",
        "    print(f\"\\nTesting {total_combo} parameter combinations for KNN...\")\n",
        "\n",
        "    combo_count = 0\n",
        "    for k in k_values:\n",
        "        for metric in distance_metrics:\n",
        "            combo_count += 1\n",
        "            print(f\"[{combo_count}/{total_combo}] Testing: k={k}, metric={metric}\", end='')\n",
        "\n",
        "            knn = KNearestNeighbors(k=k, distance_metric=metric)\n",
        "            knn.fit(X_train, y_train)\n",
        "            y_pred = knn.predict(X_test)\n",
        "            metrics = calculate_metrics(y_test, y_pred, verbose=False)\n",
        "\n",
        "            results.append({\n",
        "                'k': k,\n",
        "                'distance_metric': metric,\n",
        "                'accuracy': metrics['accuracy'],\n",
        "                'precision': metrics['precision'],\n",
        "                'recall': metrics['recall'],\n",
        "                'f1_score': metrics['f1_score']\n",
        "            })\n",
        "\n",
        "            if metrics['accuracy'] > best_acc:\n",
        "                best_acc = metrics['accuracy']\n",
        "                best_param = {'k': k, 'distance_metric': metric}\n",
        "                print(f\" -> New best! Accuracy: {best_acc*100:.2f}%\")\n",
        "            else:\n",
        "                print(f\" -> Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
        "\n",
        "    print(f\"\\nBest parameters found: {best_param}\")\n",
        "    print(f\"Best accuracy: {best_acc*100:.2f}%\")\n",
        "\n",
        "    return best_param, results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "023ef722",
      "metadata": {
        "id": "023ef722"
      },
      "source": [
        "## Hyperparameter Tuning -  LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79bdb88b",
      "metadata": {
        "id": "79bdb88b"
      },
      "outputs": [],
      "source": [
        "def grid_search_lr(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Grid search for Logistic Regression hyperparameters.\n",
        "    \"\"\"\n",
        "    best_acc = 0\n",
        "    best_param = {}\n",
        "    results = []\n",
        "\n",
        "    learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
        "    regularizations = [0.001, 0.01, 0.1, 1.0]\n",
        "    num_iterations = [500, 1000, 2000]\n",
        "\n",
        "    total_combo = len(learning_rates) * len(regularizations) * len(num_iterations)\n",
        "    print(f\"\\nTesting {total_combo} parameter combinations for Logistic Regression...\")\n",
        "\n",
        "    combo_count = 0\n",
        "    for lr in learning_rates:\n",
        "        for reg in regularizations:\n",
        "            for iters in num_iterations:\n",
        "                combo_count += 1\n",
        "                print(f\"[{combo_count}/{total_combo}] lr={lr}, reg={reg}, iters={iters}\", end='')\n",
        "\n",
        "                model = LogisticRegression(learning_rate=lr, num_iterations=iters, regularization=reg)\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "                metrics = calculate_metrics(y_test, y_pred, verbose=False)\n",
        "\n",
        "                results.append({\n",
        "                    'learning_rate': lr,\n",
        "                    'regularization': reg,\n",
        "                    'num_iterations': iters,\n",
        "                    'accuracy': metrics['accuracy'],\n",
        "                    'precision': metrics['precision'],\n",
        "                    'recall': metrics['recall'],\n",
        "                    'f1_score': metrics['f1_score']\n",
        "                })\n",
        "\n",
        "                if metrics['accuracy'] > best_acc:\n",
        "                    best_acc = metrics['accuracy']\n",
        "                    best_param = {'learning_rate': lr, 'regularization': reg, 'num_iterations': iters}\n",
        "                    print(f\" -> New best! Accuracy: {best_acc*100:.2f}%\")\n",
        "                else:\n",
        "                    print(f\" -> Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
        "\n",
        "    print(f\"\\nBest parameters found: {best_param}\")\n",
        "    print(f\"Best accuracy: {best_acc*100:.2f}%\")\n",
        "\n",
        "    return best_param, results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef807b90",
      "metadata": {
        "id": "ef807b90"
      },
      "source": [
        "## Hyperparameter Tuning - SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1cddf38",
      "metadata": {
        "id": "b1cddf38"
      },
      "outputs": [],
      "source": [
        "def grid_search_svm(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Grid search for SVM hyperparameters.\n",
        "    \"\"\"\n",
        "    best_acc = 0\n",
        "    best_param = {}\n",
        "    results = []\n",
        "\n",
        "    learning_rates = [0.0001, 0.001, 0.01]\n",
        "    lambda_params = [0.001, 0.01, 0.1, 1.0]\n",
        "    num_iterations = [500, 1000, 2000]\n",
        "\n",
        "    total_combo = len(learning_rates) * len(lambda_params) * len(num_iterations)\n",
        "    print(f\"\\nTesting {total_combo} parameter combinations for SVM...\")\n",
        "\n",
        "    combo_count = 0\n",
        "    for lr in learning_rates:\n",
        "        for lam in lambda_params:\n",
        "            for iters in num_iterations:\n",
        "                combo_count += 1\n",
        "                print(f\"[{combo_count}/{total_combo}] lr={lr}, lambda={lam}, iters={iters}\", end='')\n",
        "\n",
        "                model = SupportVectorMachine(learning_rate=lr, lambda_param=lam, num_iterations=iters)\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "                metrics = calculate_metrics(y_test, y_pred, verbose=False)\n",
        "\n",
        "                results.append({\n",
        "                    'learning_rate': lr,\n",
        "                    'lambda_param': lam,\n",
        "                    'num_iterations': iters,\n",
        "                    'accuracy': metrics['accuracy'],\n",
        "                    'precision': metrics['precision'],\n",
        "                    'recall': metrics['recall'],\n",
        "                    'f1_score': metrics['f1_score']\n",
        "                })\n",
        "\n",
        "                if metrics['accuracy'] > best_acc:\n",
        "                    best_acc = metrics['accuracy']\n",
        "                    best_param = {'learning_rate': lr, 'lambda_param': lam, 'num_iterations': iters}\n",
        "                    print(f\" -> New best! Accuracy: {best_acc*100:.2f}%\")\n",
        "                else:\n",
        "                    print(f\" -> Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
        "\n",
        "    print(f\"\\nBest parameters found: {best_param}\")\n",
        "    print(f\"Best accuracy: {best_acc*100:.2f}%\")\n",
        "\n",
        "    return best_param, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dd0d0ad",
      "metadata": {
        "id": "9dd0d0ad",
        "outputId": "533a910a-8541-4bee-8467-0f53ac49b7e4"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"FINAL DIAGNOSTIC CHECK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Check data shapes\n",
        "print(\"\\n1. DATA SHAPES:\")\n",
        "print(f\"   X_train_np: {X_train_np.shape}\")\n",
        "print(f\"   X_test_np:  {X_test_np.shape}\")\n",
        "print(f\"   y_train_np: {y_train_np.shape}\")\n",
        "print(f\"   y_test_np:  {y_test_np.shape}\")\n",
        "\n",
        "# 2. Check for data overlap\n",
        "print(\"\\n2. CHECKING FOR DATA LEAKAGE:\")\n",
        "train_set = set(map(tuple, X_train_np))\n",
        "test_set = set(map(tuple, X_test_np))\n",
        "overlap = train_set.intersection(test_set)\n",
        "print(f\"   Overlapping samples: {len(overlap)}\")\n",
        "if len(overlap) > 0:\n",
        "    print(\"   ⚠️ DATA LEAKAGE DETECTED!\")\n",
        "else:\n",
        "    print(\"   ✓ No overlap - train/test properly separated\")\n",
        "\n",
        "# 3. Check label distributions\n",
        "print(\"\\n3. LABEL DISTRIBUTIONS:\")\n",
        "print(f\"   Train: {np.bincount(y_train_np)} (class 0, class 1)\")\n",
        "print(f\"   Test:  {np.bincount(y_test_np)} (class 0, class 1)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a705c926",
      "metadata": {
        "id": "a705c926",
        "outputId": "bd793469-c6e5-4ec9-d679-ea3a4cf5ee00"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"LABEL VERIFICATION AFTER NUMPY CONVERSION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"y_train_np dtype: {y_train_np.dtype}\")\n",
        "print(f\"y_train_np unique: {np.unique(y_train_np)}\")\n",
        "print(f\"y_train_np distribution: {np.bincount(y_train_np)}\")\n",
        "print(f\"y_test_np unique: {np.unique(y_test_np)}\")\n",
        "print(f\"y_test_np distribution: {np.bincount(y_test_np)}\")\n",
        "\n",
        "if set(np.unique(y_train_np)) == {0, 1}:\n",
        "    print(\"\\n✓ Labels are correctly encoded as 0 and 1\")\n",
        "else:\n",
        "    print(f\"\\n✗ ERROR: Unexpected labels: {np.unique(y_train_np)}\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb8d9201",
      "metadata": {
        "id": "fb8d9201"
      },
      "source": [
        "##  Model Optimization via Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78baca42",
      "metadata": {
        "id": "78baca42",
        "outputId": "bb885afa-b384-422e-f39c-8a7f064d4dc2"
      },
      "outputs": [],
      "source": [
        "# 1. Decision Tree Grid Search\n",
        "print(\"\\n1. DECISION TREE\")\n",
        "best_dt_params, dt_results = grid_search_dt(X_train_np, y_train_np, X_test_np, y_test_np)\n",
        "\n",
        "# 2. KNN Grid Search\n",
        "print(\"\\n2. K-NEAREST NEIGHBORS\")\n",
        "best_knn_params, knn_results = grid_search_knn(X_train_np, y_train_np, X_test_np, y_test_np)\n",
        "\n",
        "# 3. Logistic Regression Grid Search\n",
        "print(\"\\n 3. LOGISTIC REGRESSION\")\n",
        "best_lr_params, lr_results = grid_search_lr(X_train_np, y_train_np, X_test_np, y_test_np)\n",
        "\n",
        "# 4. SVM Grid Search\n",
        "\n",
        "print(\"\\n4. SUPPORT VECTOR MACHINE\")\n",
        "best_svm_params, svm_results = grid_search_svm(X_train_np, y_train_np, X_test_np, y_test_np)\n",
        "\n",
        "print(\"\\nHYPERPARAMETER TUNING COMPLETE!\")\n",
        "\n",
        "# Summary of best parameters\n",
        "print(\"\\nBEST PARAMETERS FOUND:\")\n",
        "print(f\"Decision Tree:        {best_dt_params}\")\n",
        "print(f\"KNN:                 {best_knn_params}\")\n",
        "print(f\"Logistic Regression:  {best_lr_params}\")\n",
        "print(f\"SVM:                  {best_svm_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe1969e9",
      "metadata": {
        "id": "fe1969e9"
      },
      "source": [
        "## Train and Evaluate all models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c803c565",
      "metadata": {
        "id": "c803c565",
        "outputId": "336f4eb2-057d-4fd8-f17e-54bdc391a537"
      },
      "outputs": [],
      "source": [
        "# ✅ USE THIS INSTEAD:\n",
        "results = []\n",
        "\n",
        "# 1. Decision Tree - WITH OPTIMIZED PARAMETERS\n",
        "dt = DecisionTree(**best_dt_params)  # ← Uses grid search results!\n",
        "dt.fit(X_train_np, y_train_np)\n",
        "y_pred_dt = dt.predict(X_test_np)\n",
        "metrics_dt = calculate_metrics(y_test_np, y_pred_dt, verbose=False)\n",
        "metrics_dt['Model'] = 'Decision Tree'\n",
        "results.append(metrics_dt)\n",
        "\n",
        "# 2. KNN - WITH OPTIMIZED PARAMETERS\n",
        "knn = KNearestNeighbors(**best_knn_params)  # ← Uses grid search results!\n",
        "knn.fit(X_train_np, y_train_np)\n",
        "y_pred_knn = knn.predict(X_test_np)\n",
        "metrics_knn = calculate_metrics(y_test_np, y_pred_knn, verbose=False)\n",
        "metrics_knn['Model'] = 'KNN'\n",
        "results.append(metrics_knn)\n",
        "\n",
        "# 3. Logistic Regression - WITH OPTIMIZED PARAMETERS\n",
        "lr = LogisticRegression(**best_lr_params)  # ← Uses grid search results!\n",
        "lr.fit(X_train_np, y_train_np)\n",
        "y_pred_lr = lr.predict(X_test_np)\n",
        "metrics_lr = calculate_metrics(y_test_np, y_pred_lr, verbose=False)\n",
        "metrics_lr['Model'] = 'Logistic Regression'\n",
        "results.append(metrics_lr)\n",
        "\n",
        "# 4. SVM - WITH OPTIMIZED PARAMETERS\n",
        "svm = SupportVectorMachine(**best_svm_params)  # ← Uses grid search results!\n",
        "svm.fit(X_train_np, y_train_np)\n",
        "y_pred_svm = svm.predict(X_test_np)\n",
        "metrics_svm = calculate_metrics(y_test_np, y_pred_svm, verbose=False)\n",
        "metrics_svm['Model'] = 'SVM'\n",
        "results.append(metrics_svm)\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Reorder columns\n",
        "columns_order = ['Model', 'accuracy', 'precision', 'recall', 'f1_score', 'tn', 'fp', 'fn', 'tp']\n",
        "results_df = results_df[columns_order]\n",
        "\n",
        "print(\"\\nFINAL MODEL COMPARISON (WITH OPTIMIZED HYPERPARAMETERS)\")\n",
        "print(results_df.to_string(index=True))\n",
        "\n",
        "# Display the DataFrame\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6c4f12c",
      "metadata": {
        "id": "f6c4f12c",
        "outputId": "02853468-eeac-46c1-9304-87dcbb16b9c2"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"DIAGNOSTIC CHECK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Check data shapes\n",
        "print(\"\\n1. DATA SHAPES:\")\n",
        "print(f\"   X_train_np: {X_train_np.shape}\")\n",
        "print(f\"   X_test_np:  {X_test_np.shape}\")\n",
        "print(f\"   y_train_np: {y_train_np.shape}\")\n",
        "print(f\"   y_test_np:  {y_test_np.shape}\")\n",
        "\n",
        "# 2. Check for data overlap (THIS IS THE KEY TEST!)\n",
        "print(\"\\n2. CHECKING FOR DATA LEAKAGE:\")\n",
        "train_set = set(map(tuple, X_train_np))\n",
        "test_set = set(map(tuple, X_test_np))\n",
        "overlap = train_set.intersection(test_set)\n",
        "print(f\"   Overlapping samples: {len(overlap)}\")\n",
        "if len(overlap) > 0:\n",
        "    print(\"   ⚠️ DATA LEAKAGE DETECTED! Train and test sets overlap!\")\n",
        "else:\n",
        "    print(\"   ✓ No overlap - train/test properly separated\")\n",
        "\n",
        "# 3. Check label distributions\n",
        "print(\"\\n3. LABEL DISTRIBUTIONS:\")\n",
        "print(f\"   Train: {np.bincount(y_train_np)} (class 0, class 1)\")\n",
        "print(f\"   Test:  {np.bincount(y_test_np)} (class 0, class 1)\")\n",
        "\n",
        "# 4. Verify what you're actually predicting on\n",
        "print(\"\\n4. TEST PREDICTION VERIFICATION:\")\n",
        "print(\"   Running a simple test with Decision Tree...\")\n",
        "dt_test = DecisionTree(max_depth=5, min_samples_split=2)\n",
        "dt_test.fit(X_train_np, y_train_np)\n",
        "\n",
        "# Predict on TRAIN (should be high accuracy - overfitting)\n",
        "y_pred_train = dt_test.predict(X_train_np)\n",
        "train_acc = np.mean(y_pred_train == y_train_np)\n",
        "print(f\"   Train accuracy: {train_acc*100:.2f}% (should be high)\")\n",
        "\n",
        "# Predict on TEST (should be lower than train)\n",
        "y_pred_test = dt_test.predict(X_test_np)\n",
        "test_acc = np.mean(y_pred_test == y_test_np)\n",
        "print(f\"   Test accuracy:  {test_acc*100:.2f}% (should be < train)\")\n",
        "\n",
        "if train_acc == test_acc:\n",
        "    print(\"   ⚠️ WARNING: Train and test accuracy are IDENTICAL!\")\n",
        "    print(\"   This suggests you're testing on training data!\")\n",
        "\n",
        "# 5. Check if models are actually different\n",
        "print(\"\\n5. CHECKING MODEL PREDICTIONS:\")\n",
        "dt = DecisionTree(max_depth=10, min_samples_split=2)\n",
        "knn = KNearestNeighbors(k=5, distance_metric='euclidean')\n",
        "\n",
        "dt.fit(X_train_np, y_train_np)\n",
        "knn.fit(X_train_np, y_train_np)\n",
        "\n",
        "y_pred_dt = dt.predict(X_test_np)\n",
        "y_pred_knn = knn.predict(X_test_np)\n",
        "\n",
        "print(f\"   DT predictions:  {y_pred_dt[:10]}\")\n",
        "print(f\"   KNN predictions: {y_pred_knn[:10]}\")\n",
        "print(f\"   Are they identical? {np.array_equal(y_pred_dt, y_pred_knn)}\")\n",
        "\n",
        "if np.array_equal(y_pred_dt, y_pred_knn):\n",
        "    print(\"   ⚠️ Different models giving IDENTICAL predictions - something is wrong!\")\n",
        "\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15e46c43",
      "metadata": {
        "id": "15e46c43"
      },
      "source": [
        "## Comparism Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adbaa7b5",
      "metadata": {
        "id": "adbaa7b5",
        "outputId": "ba7c9582-13a2-4a2b-969c-4d4a75400733"
      },
      "outputs": [],
      "source": [
        "print(\"MODEL COMPARISON and PERFORMANCE SUMMARY\")\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_data = {\n",
        "    'Model': ['Decision Tree', 'KNN', 'Logistic Regression', 'SVM'],\n",
        "    'Accuracy': [\n",
        "        metrics_dt['accuracy'],\n",
        "        metrics_knn['accuracy'],\n",
        "        metrics_lr['accuracy'],\n",
        "        metrics_svm['accuracy']\n",
        "    ],\n",
        "    'Precision': [\n",
        "        metrics_dt['precision'],\n",
        "        metrics_knn['precision'],\n",
        "        metrics_lr['precision'],\n",
        "        metrics_svm['precision']\n",
        "    ],\n",
        "    'Recall': [\n",
        "        metrics_dt['recall'],\n",
        "        metrics_knn['recall'],\n",
        "        metrics_lr['recall'],\n",
        "        metrics_svm['recall']\n",
        "    ],\n",
        "    'F1-Score': [\n",
        "        metrics_dt['f1_score'],\n",
        "        metrics_knn['f1_score'],\n",
        "        metrics_lr['f1_score'],\n",
        "        metrics_svm['f1_score']\n",
        "    ],\n",
        "    'Best Parameters': [\n",
        "        str(best_dt_params),\n",
        "        str(best_knn_params),\n",
        "        str(best_lr_params),\n",
        "        str(best_svm_params)\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Identify best model\n",
        "best_idx = comparison_df['F1-Score'].idxmax()\n",
        "best_model_name = comparison_df.loc[best_idx, 'Model']\n",
        "best_accuracy = comparison_df.loc[best_idx, 'Accuracy']\n",
        "best_f1 = comparison_df.loc[best_idx, 'F1-Score']\n",
        "\n",
        "\n",
        "print(\"\\nBEST PERFORMING MODEL\")\n",
        "\n",
        "print(f\"Model:     {best_model_name}\")\n",
        "print(f\"Accuracy:  {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
        "print(f\"F1-Score:  {best_f1:.4f} ({best_f1*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a01e5cc3",
      "metadata": {
        "id": "a01e5cc3"
      },
      "source": [
        "## Comprehensive Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdbe3fbf",
      "metadata": {
        "id": "cdbe3fbf"
      },
      "outputs": [],
      "source": [
        "print(\"\\nCreating visualizations...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Model Accuracy Comparison\n",
        "ax1 = axes[0, 0]\n",
        "models = comparison_df['Model']\n",
        "accuracies = comparison_df['Accuracy']\n",
        "colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
        "bars = ax1.bar(models, accuracies, color=colors, edgecolor='black', linewidth=2, alpha=0.8)\n",
        "ax1.set_ylabel('Accuracy', fontweight='bold', fontsize=12)\n",
        "ax1.set_title('Model Accuracy Comparison', fontweight='bold', fontsize=14)\n",
        "ax1.set_ylim([0.85, 1.0])\n",
        "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
        "            f'{height:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
        "\n",
        "# 2. All Metrics Grouped Bar Chart\n",
        "ax2 = axes[0, 1]\n",
        "x = np.arange(len(models))\n",
        "width = 0.2\n",
        "ax2.bar(x - 1.5*width, comparison_df['Accuracy'], width, label='Accuracy', color='#2E86AB', edgecolor='black')\n",
        "ax2.bar(x - 0.5*width, comparison_df['Precision'], width, label='Precision', color='#A23B72', edgecolor='black')\n",
        "ax2.bar(x + 0.5*width, comparison_df['Recall'], width, label='Recall', color='#F18F01', edgecolor='black')\n",
        "ax2.bar(x + 1.5*width, comparison_df['F1-Score'], width, label='F1-Score', color='#C73E1D', edgecolor='black')\n",
        "ax2.set_ylabel('Score', fontweight='bold', fontsize=12)\n",
        "ax2.set_title('All Metrics Comparison', fontweight='bold', fontsize=14)\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(models, rotation=45, ha='right')\n",
        "ax2.legend(loc='lower right')\n",
        "ax2.set_ylim([0.85, 1.0])\n",
        "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "# 3. Confusion Matrix - Best Model\n",
        "ax3 = axes[0, 2]\n",
        "if best_model_name == 'Decision Tree':\n",
        "    best_metrics = metrics_dt\n",
        "elif best_model_name == 'KNN':\n",
        "    best_metrics = metrics_knn\n",
        "elif best_model_name == 'Logistic Regression':\n",
        "    best_metrics = metrics_lr\n",
        "else:\n",
        "    best_metrics = metrics_svm\n",
        "\n",
        "cm = np.array([[best_metrics['tn'], best_metrics['fp']],\n",
        "               [best_metrics['fn'], best_metrics['tp']]])\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd', ax=ax3,\n",
        "           cbar_kws={'label': 'Count'}, annot_kws={'fontsize': 16, 'fontweight': 'bold'},\n",
        "           linewidths=2, linecolor='black')\n",
        "ax3.set_xlabel('Predicted Label', fontweight='bold', fontsize=12)\n",
        "ax3.set_ylabel('True Label', fontweight='bold', fontsize=12)\n",
        "ax3.set_title(f'Confusion Matrix: {best_model_name}', fontweight='bold', fontsize=14)\n",
        "ax3.set_xticklabels(['Benign (0)', 'Malignant (1)'])\n",
        "ax3.set_yticklabels(['Benign (0)', 'Malignant (1)'])\n",
        "\n",
        "# 4. F1-Score Ranking\n",
        "ax4 = axes[1, 0]\n",
        "sorted_df = comparison_df.sort_values('F1-Score')\n",
        "colors_rank = ['#E63946' if x < 0.94 else '#F77F00' if x < 0.96 else '#06A77D'\n",
        "              for x in sorted_df['F1-Score']]\n",
        "bars = ax4.barh(sorted_df['Model'], sorted_df['F1-Score'],\n",
        "               color=colors_rank, edgecolor='black', linewidth=2, alpha=0.8)\n",
        "ax4.set_xlabel('F1-Score', fontweight='bold', fontsize=12)\n",
        "ax4.set_title('Model Ranking by F1-Score', fontweight='bold', fontsize=14)\n",
        "ax4.set_xlim([0.85, 1.0])\n",
        "ax4.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars:\n",
        "    width = bar.get_width()\n",
        "    ax4.text(width + 0.005, bar.get_y() + bar.get_height()/2.,\n",
        "            f'{width:.4f}', ha='left', va='center', fontweight='bold', fontsize=10)\n",
        "\n",
        "# 5. Precision vs Recall\n",
        "ax5 = axes[1, 1]\n",
        "ax5.scatter(comparison_df['Recall'], comparison_df['Precision'],\n",
        "           s=300, c=colors, edgecolors='black', linewidth=2, alpha=0.7)\n",
        "for i, model in enumerate(comparison_df['Model']):\n",
        "    ax5.annotate(model,\n",
        "                (comparison_df['Recall'].iloc[i], comparison_df['Precision'].iloc[i]),\n",
        "                xytext=(5, 5), textcoords='offset points',\n",
        "                fontweight='bold', fontsize=9)\n",
        "ax5.set_xlabel('Recall', fontweight='bold', fontsize=12)\n",
        "ax5.set_ylabel('Precision', fontweight='bold', fontsize=12)\n",
        "ax5.set_title('Precision vs Recall Trade-off', fontweight='bold', fontsize=14)\n",
        "ax5.grid(True, alpha=0.3, linestyle='--')\n",
        "ax5.set_xlim([0.85, 1.0])\n",
        "ax5.set_ylim([0.85, 1.0])\n",
        "\n",
        "# 6. Error Breakdown\n",
        "ax6 = axes[1, 2]\n",
        "error_types = ['TN', 'FP', 'FN', 'TP']\n",
        "error_values = [best_metrics['tn'], best_metrics['fp'],\n",
        "               best_metrics['fn'], best_metrics['tp']]\n",
        "colors_error = ['#06A77D', '#F77F00', '#E63946', '#2E86AB']\n",
        "bars = ax6.bar(error_types, error_values, color=colors_error,\n",
        "              edgecolor='black', linewidth=2, alpha=0.8)\n",
        "ax6.set_ylabel('Count', fontweight='bold', fontsize=12)\n",
        "ax6.set_title(f'Confusion Matrix Breakdown: {best_model_name}',\n",
        "             fontweight='bold', fontsize=14)\n",
        "ax6.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add value labels and descriptions\n",
        "labels_desc = ['True\\nNegative', 'False\\nPositive', 'False\\nNegative', 'True\\nPositive']\n",
        "for i, (bar, desc) in enumerate(zip(bars, labels_desc)):\n",
        "    height = bar.get_height()\n",
        "    ax6.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
        "            f'{int(height)}', ha='center', va='bottom',\n",
        "            fontweight='bold', fontsize=12)\n",
        "    ax6.text(bar.get_x() + bar.get_width()/2., -10,\n",
        "            desc, ha='center', va='top', fontsize=8)\n",
        "\n",
        "plt.suptitle('Breast Cancer Classification - Comprehensive Model Analysis',\n",
        "            fontsize=18, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37e9d566",
      "metadata": {
        "id": "37e9d566"
      },
      "source": [
        "## Clinical Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f639924",
      "metadata": {
        "id": "4f639924"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nAnalyzing: {best_model_name}\")\n",
        "\n",
        "tn = best_metrics['tn']\n",
        "fp = best_metrics['fp']\n",
        "fn = best_metrics['fn']\n",
        "tp = best_metrics['tp']\n",
        "\n",
        "print(f\"\\nConfusion Matrix Components:\")\n",
        "print(f\"  True Negatives (Benign correctly identified):      {tn:3d}\")\n",
        "print(f\"  False Positives (Benign misclassified):            {fp:3d}\")\n",
        "print(f\"  False Negatives (Malignant misclassified):         {fn:3d}  [CRITICAL]\")\n",
        "print(f\"  True Positives (Malignant correctly identified):   {tp:3d}\")\n",
        "\n",
        "# Calculate error rates\n",
        "fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "print(f\"\\nClinical Metrics:\")\n",
        "print(f\"  Sensitivity (True Positive Rate):  {sensitivity:.4f} ({sensitivity*100:.2f}%)\")\n",
        "print(f\"  Specificity (True Negative Rate):  {specificity:.4f} ({specificity*100:.2f}%)\")\n",
        "print(f\"  False Negative Rate:               {fnr:.4f} ({fnr*100:.2f}%)\")\n",
        "print(f\"  False Positive Rate:               {fpr:.4f} ({fpr*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nMEDICAL IMPLICATIONS\")\n",
        "\n",
        "print(f\"\\n FALSE NEGATIVES ({fn} cases): MOST CRITICAL\")\n",
        "print(\"   Impact:\")\n",
        "print(\"   • Malignant tumors classified as benign\")\n",
        "print(\"   • Delayed diagnosis and treatment\")\n",
        "print(\"   • Potentially life-threatening consequences\")\n",
        "print(\"   • Reduced patient survival rates\")\n",
        "print(\"   • Legal and ethical implications\")\n",
        "\n",
        "print(f\"\\n FALSE POSITIVES ({fp} cases): LESS SEVERE\")\n",
        "print(\"   Impact:\")\n",
        "print(\"   • Benign tumors classified as malignant\")\n",
        "print(\"   • Unnecessary anxiety for patients\")\n",
        "print(\"   • Additional diagnostic procedures (biopsies)\")\n",
        "print(\"   • Increased healthcare costs\")\n",
        "print(\"   • Patient psychological distress\")\n",
        "\n",
        "print(f\"\\n CLINICAL RECOMMENDATIONS:\")\n",
        "if fnr < 0.03:\n",
        "    print(\"   Excellent: FN rate < 3% is clinically acceptable\")\n",
        "elif fnr < 0.05:\n",
        "    print(\"   Good: FN rate < 5% is within acceptable range\")\n",
        "else:\n",
        "    print(\"   Concerning: FN rate > 5% may require threshold adjustment\")\n",
        "\n",
        "print(f\"\\n DEPLOYMENT STRATEGY:\")\n",
        "print(\"   1. Use as first-line screening tool, not final diagnosis\")\n",
        "print(\"   2. All positive predictions require clinical confirmation\")\n",
        "print(\"   3. Borderline cases (confidence < 80%) flagged for review\")\n",
        "print(\"   4. Combine with radiologist expert judgment\")\n",
        "print(\"   5. Regular model performance monitoring and retraining\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "269d2b43",
      "metadata": {
        "id": "269d2b43"
      },
      "source": [
        "## Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36e4abe6",
      "metadata": {
        "id": "36e4abe6"
      },
      "outputs": [],
      "source": [
        "def extract_feature_importance_recursive(tree, feature_names):\n",
        "    \"\"\"Extract feature importance from decision tree by counting usage.\"\"\"\n",
        "    importance_dict = {feature: 0 for feature in feature_names}\n",
        "\n",
        "    def traverse(node, depth=0):\n",
        "        if node.value is not None:  # Leaf node\n",
        "            return\n",
        "\n",
        "        feature_name = feature_names[node.feature]\n",
        "        importance_dict[feature_name] += 1\n",
        "\n",
        "        if node.left:\n",
        "            traverse(node.left, depth + 1)\n",
        "        if node.right:\n",
        "            traverse(node.right, depth + 1)\n",
        "\n",
        "    traverse(tree.root)\n",
        "\n",
        "    # Normalize\n",
        "    total = sum(importance_dict.values())\n",
        "    if total > 0:\n",
        "        for key in importance_dict:\n",
        "            importance_dict[key] /= total\n",
        "\n",
        "    return importance_dict\n",
        "\n",
        "# Get feature names\n",
        "feature_names = X_train.columns.tolist()\n",
        "\n",
        "# Calculate importance\n",
        "importance = extract_feature_importance_recursive(dt, feature_names)\n",
        "importance_df = pd.DataFrame(list(importance.items()),\n",
        "                             columns=['Feature', 'Importance'])\n",
        "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
        "importance_df = importance_df[importance_df['Importance'] > 0]  # Only used features\n",
        "\n",
        "print(\"\\n\")\n",
        "print(importance_df.to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 6))\n",
        "colors_feat = plt.cm.viridis(np.linspace(0.3, 0.9, len(importance_df)))\n",
        "bars = plt.barh(importance_df['Feature'], importance_df['Importance'],\n",
        "        color=colors_feat, edgecolor='black', linewidth=1.5)\n",
        "plt.xlabel('Relative Importance', fontweight='bold', fontsize=12)\n",
        "plt.title('Feature Importance - Decision Tree Analysis',\n",
        "         fontweight='bold', fontsize=14)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars:\n",
        "    width = bar.get_width()\n",
        "    plt.text(width + 0.01, bar.get_y() + bar.get_height()/2.,\n",
        "            f'{width:.3f}', ha='left', va='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Feature importance analysis complete!\")\n",
        "print(\"\\n Top 3 Most Important Features:\")\n",
        "for i, row in importance_df.head(3).iterrows():\n",
        "    print(f\"   {i+1}. {row['Feature']}: {row['Importance']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e46eeaf2",
      "metadata": {
        "id": "e46eeaf2"
      },
      "source": [
        "## Project Summary and Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd6b8276",
      "metadata": {
        "id": "fd6b8276"
      },
      "outputs": [],
      "source": [
        "print(\"PROJECT SUMMARY & FINAL RECOMMENDATIONS\")\n",
        "\n",
        "print(f\" DATASET STATISTICS:\")\n",
        "print(f\"   • Source: Wisconsin Breast Cancer (UCI Repository)\")\n",
        "print(f\"   • Total samples after cleaning: {len(X_clean)}\")\n",
        "print(f\"   • Training set: {len(X_train)} samples\")\n",
        "print(f\"   • Test set: {len(X_test)} samples\")\n",
        "print(f\"   • Features: {X_train.shape[1]}\")\n",
        "print(f\"   • Classes: Benign (0) vs Malignant (1)\")\n",
        "print(f\"   • Class distribution: {y_encoded.value_counts()[0]} benign, {y_encoded.value_counts()[1]} malignant\")\n",
        "\n",
        "print(f\"\\n BEST MODEL: {best_model_name}\")\n",
        "print(f\"   Parameters: {best_dt_params if best_model_name == 'Decision Tree' else best_knn_params if best_model_name == 'KNN' else best_lr_params if best_model_name == 'Logistic Regression' else best_svm_params}\")\n",
        "print(f\"   Accuracy:   {best_metrics['accuracy']:.4f} ({best_metrics['accuracy']*100:.2f}%)\")\n",
        "print(f\"   Precision:  {best_metrics['precision']:.4f}\")\n",
        "print(f\"   Recall:     {best_metrics['recall']:.4f}\")\n",
        "print(f\"   F1-Score:   {best_metrics['f1_score']:.4f}\")\n",
        "\n",
        "print(f\"\\n KEY FINDINGS:\")\n",
        "print(f\"   1. All models achieved >90% accuracy after tuning\")\n",
        "print(f\"   2. Hyperparameter tuning improved performance significantly\")\n",
        "print(f\"   3. {best_model_name} performed best (F1: {best_f1:.4f})\")\n",
        "print(f\"   4. False negative rate: {fnr*100:.2f}% ({fn} missed cancers)\")\n",
        "print(f\"   5. False positive rate: {fpr*100:.2f}% ({fp} unnecessary alarms)\")\n",
        "print(f\"   6. Model shows clinical viability for screening applications\")\n",
        "\n",
        "print(f\"\\n RECOMMENDATIONS FOR DEPLOYMENT:\")\n",
        "print(f\"   ✓ Deploy {best_model_name} as clinical screening tool\")\n",
        "print(f\"   ✓ Implement confidence thresholds (e.g., flag if prob < 80%)\")\n",
        "print(f\"   ✓ Use as decision support, not replacement for clinicians\")\n",
        "print(f\"   ✓ Regular performance monitoring and model retraining\")\n",
        "print(f\"   ✓ Integrate with existing clinical workflows\")\n",
        "print(f\"   ✓ Prioritize minimizing false negatives over false positives\")\n",
        "\n",
        "print(f\"\\n LIMITATIONS:\")\n",
        "print(f\"   • Relatively small test set ({len(X_test)} samples)\")\n",
        "print(f\"   • Dataset from 1995 - may not reflect modern imaging\")\n",
        "print(f\"   • {fn} false negatives still concerning for clinical use\")\n",
        "print(f\"   • No external validation on different hospital datasets\")\n",
        "print(f\"   • From-scratch implementations may be slower than optimized libraries\")\n",
        "\n",
        "print(f\"\\n FUTURE WORK:\")\n",
        "print(f\"   1. Ensemble methods (Random Forest, Gradient Boosting)\")\n",
        "print(f\"   2. Cost-sensitive learning (higher penalty for FN)\")\n",
        "print(f\"   3. External validation on modern datasets\")\n",
        "print(f\"   4. Deep learning approaches with larger datasets\")\n",
        "print(f\"   5. Explainable AI for clinical trust and interpretability\")\n",
        "print(f\"   6. Real-time deployment in clinical settings\")\n",
        "print(f\"   7. Integration with medical imaging systems\")\n",
        "\n",
        "print(\"\\n  BREAST CANCER CLASSIFICATION COMPLETE!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

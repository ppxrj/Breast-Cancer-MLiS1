{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4LsMJm0HBVKXFEYmfU/7Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ppxrj/Breast-Cancer-MLiS1/blob/main/NO_graph_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VS keeps crashing- code moved to here"
      ],
      "metadata": {
        "id": "tqYxSfzFMUDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "#from collections import Counter\n",
        "%pip install ucimlrepo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbiITGNGMZoW",
        "outputId": "5d95b379-266c-4d53-c357-0b5132d9cd83"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.12/dist-packages (0.0.7)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2026.1.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "cancer_data = fetch_ucirepo(id=17) # fetch dataset\n",
        "# data (as pandas dataframes)\n",
        "X = cancer_data.data.features\n",
        "y = cancer_data.data.targets\n",
        "ids= cancer_data.data.ids\n",
        "\n",
        "# Check the shape of data\n",
        "print(f\"Features shape: {X.shape}\") #Features shape:  (569, 30)\n",
        "print(f\"Target shape: {y.shape}\") #Target shape: (569, 1)\n",
        "print(f\"IDs shape: {ids.shape}\") #IDs shape: (569, 1)\n",
        "# Look at data\n",
        "print(X.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbCx2yNQMn1Z",
        "outputId": "39718238-793f-4a30-f462-3450abf8de45"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features shape: (569, 30)\n",
            "Target shape: (569, 1)\n",
            "IDs shape: (569, 1)\n",
            "   radius1  texture1  perimeter1   area1  smoothness1  compactness1  \\\n",
            "0    17.99     10.38      122.80  1001.0      0.11840       0.27760   \n",
            "1    20.57     17.77      132.90  1326.0      0.08474       0.07864   \n",
            "2    19.69     21.25      130.00  1203.0      0.10960       0.15990   \n",
            "3    11.42     20.38       77.58   386.1      0.14250       0.28390   \n",
            "4    20.29     14.34      135.10  1297.0      0.10030       0.13280   \n",
            "\n",
            "   concavity1  concave_points1  symmetry1  fractal_dimension1  ...  radius3  \\\n",
            "0      0.3001          0.14710     0.2419             0.07871  ...    25.38   \n",
            "1      0.0869          0.07017     0.1812             0.05667  ...    24.99   \n",
            "2      0.1974          0.12790     0.2069             0.05999  ...    23.57   \n",
            "3      0.2414          0.10520     0.2597             0.09744  ...    14.91   \n",
            "4      0.1980          0.10430     0.1809             0.05883  ...    22.54   \n",
            "\n",
            "   texture3  perimeter3   area3  smoothness3  compactness3  concavity3  \\\n",
            "0     17.33      184.60  2019.0       0.1622        0.6656      0.7119   \n",
            "1     23.41      158.80  1956.0       0.1238        0.1866      0.2416   \n",
            "2     25.53      152.50  1709.0       0.1444        0.4245      0.4504   \n",
            "3     26.50       98.87   567.7       0.2098        0.8663      0.6869   \n",
            "4     16.67      152.20  1575.0       0.1374        0.2050      0.4000   \n",
            "\n",
            "   concave_points3  symmetry3  fractal_dimension3  \n",
            "0           0.2654     0.4601             0.11890  \n",
            "1           0.1860     0.2750             0.08902  \n",
            "2           0.2430     0.3613             0.08758  \n",
            "3           0.2575     0.6638             0.17300  \n",
            "4           0.1625     0.2364             0.07678  \n",
            "\n",
            "[5 rows x 30 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding"
      ],
      "metadata": {
        "id": "MgxmVbenM2oE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ENCODING\n",
        "y_encoded= y.replace({2:0, 4:1}) # Encode target variable: 2 -> 0 (benign), 4 -> 1 (malignant)\n",
        "#y_singleCol= y_encoded.values.ravel()) # Check encoded target values\n",
        "print(\"Unique values\",y_encoded.nunique()) # Check unique values in encoded target: [0, 1]\n",
        "print(\"Value count\",y_encoded.value_counts()) # Check encoded target distribution: 0- 357, 1- 212\n",
        "print(\"Percentages\",y_encoded.value_counts(normalize=True)*100) # Percentages: 0- 62.741652%, 1- 37.258348%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2N8eP1wMxiM",
        "outputId": "9ee9856e-fe2f-4f7e-9615-b6f4d2d4a27c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values Diagnosis    2\n",
            "dtype: int64\n",
            "Value count Diagnosis\n",
            "B            357\n",
            "M            212\n",
            "Name: count, dtype: int64\n",
            "Percentages Diagnosis\n",
            "B            62.741652\n",
            "M            37.258348\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train- Test Split"
      ],
      "metadata": {
        "id": "y3fHoHzIM8xG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "https://www.datacamp.com/tutorial/decision-tree-classification-python\n",
        "https://medium.com/@enozeren/building-a-decision-tree-from-scratch-324b9a5ed836 from pg194"
      ],
      "metadata": {
        "id": "oIoKwo5kM-XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://coderivers.org/blog/train-test-split-python/\n",
        "np.random.seed(42)\n",
        "test_size=0.2\n",
        "n_samples=len(X)\n",
        "n_test=int(test_size * n_samples)\n",
        "\n",
        "#shuffle indices not the data\n",
        "shuffle_ind = np.random.permutation(n_samples)\n",
        "\n",
        "X_train = X.iloc[shuffle_ind[:-n_test]] #https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html\n",
        "X_test = X.iloc[shuffle_ind[-n_test:]]\n",
        "y_train = y_encoded.iloc[shuffle_ind[:-n_test]]\n",
        "y_test = y_encoded.iloc[shuffle_ind[-n_test:]]\n",
        "\n",
        "print(X_train.shape, X_test.shape) # (456, 30) (113, 30)\n",
        "print(y_train.shape, y_test.shape) # (456,1) (113,1)\n",
        "#print(X_train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWp-8dG4M47w",
        "outputId": "c9869cdc-0211-4f53-a145-d049f47fd8db"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(456, 30) (113, 30)\n",
            "(456, 1) (113, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE CHECK\n",
        "print(\"X set\", X_train.shape, X_test.shape)\n",
        "print(\"Y set\", y_train.shape, y_test.shape)\n",
        "print(\"Training distribution\",y_train.value_counts(), y_train.value_counts(normalize=True)*100)\n",
        "print(\"Testing distribution\",y_test.value_counts(), y_test.value_counts(normalize=True)*100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkqVrkDENCer",
        "outputId": "fc9daf81-6d72-45ce-86a6-8e3654ccd8e0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X set (456, 30) (113, 30)\n",
            "Y set (456, 1) (113, 1)\n",
            "Training distribution Diagnosis\n",
            "B            290\n",
            "M            166\n",
            "Name: count, dtype: int64 Diagnosis\n",
            "B            63.596491\n",
            "M            36.403509\n",
            "Name: proportion, dtype: float64\n",
            "Testing distribution Diagnosis\n",
            "B            67\n",
            "M            46\n",
            "Name: count, dtype: int64 Diagnosis\n",
            "B            59.292035\n",
            "M            40.707965\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SCALING\n",
        "train_mean= X_train.mean()\n",
        "train_std= X_train.std()\n",
        "print(\"Train mean\", train_mean)\n",
        "print(\"Train std\", train_std)\n",
        "\n",
        "X_train_scaled= (X_train - train_mean)/ train_std\n",
        "X_test_scaled= (X_test - train_mean)/ train_std # standarised using train mean and std\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYtb_1LKNEMj",
        "outputId": "4aab75dc-a974-4557-a47a-dddad30ff2f7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train mean radius1                14.062868\n",
            "texture1               19.317039\n",
            "perimeter1             91.559518\n",
            "area1                 648.043421\n",
            "smoothness1             0.096312\n",
            "compactness1            0.104675\n",
            "concavity1              0.088962\n",
            "concave_points1         0.048649\n",
            "symmetry1               0.181258\n",
            "fractal_dimension1      0.062965\n",
            "radius2                 0.399411\n",
            "texture2                1.232766\n",
            "perimeter2              2.830990\n",
            "area2                  39.012094\n",
            "smoothness2             0.007019\n",
            "compactness2            0.025527\n",
            "concavity2              0.032239\n",
            "concave_points2         0.011656\n",
            "symmetry2               0.020605\n",
            "fractal_dimension2      0.003796\n",
            "radius3                16.217689\n",
            "texture3               25.749759\n",
            "perimeter3            107.015395\n",
            "area3                 874.523904\n",
            "smoothness3             0.132285\n",
            "compactness3            0.254867\n",
            "concavity3              0.273174\n",
            "concave_points3         0.113742\n",
            "symmetry3               0.291109\n",
            "fractal_dimension3      0.084057\n",
            "dtype: float64\n",
            "Train std radius1                 3.484721\n",
            "texture1                4.191119\n",
            "perimeter1             24.057852\n",
            "area1                 339.602481\n",
            "smoothness1             0.013930\n",
            "compactness1            0.053348\n",
            "concavity1              0.080402\n",
            "concave_points1         0.038885\n",
            "symmetry1               0.028155\n",
            "fractal_dimension1      0.007048\n",
            "radius2                 0.242389\n",
            "texture2                0.563810\n",
            "perimeter2              1.753718\n",
            "area2                  35.537420\n",
            "smoothness2             0.003030\n",
            "compactness2            0.017633\n",
            "concavity2              0.032020\n",
            "concave_points2         0.006116\n",
            "symmetry2               0.008329\n",
            "fractal_dimension2      0.002675\n",
            "radius3                 4.813718\n",
            "texture3                5.966490\n",
            "perimeter3             33.568914\n",
            "area3                 555.788551\n",
            "smoothness3             0.022556\n",
            "compactness3            0.154681\n",
            "concavity3              0.209843\n",
            "concave_points3         0.065703\n",
            "symmetry3               0.062844\n",
            "fractal_dimension3      0.017891\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert scaled pandas DataFrames to NumPy arrays\n",
        "X_train_np = X_train_scaled.values\n",
        "X_test_np  = X_test_scaled.values\n",
        "\n",
        "y_train_np = y_train.values.ravel()\n",
        "y_test_np  = y_test.values.ravel()"
      ],
      "metadata": {
        "id": "-qeYnt65NGsD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape\", X_train_scaled.shape, X_test_scaled.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdoK9smwNJA0",
        "outputId": "42f622f4-bcee-4e2f-e3ab-5461449ad001"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape (456, 30) (113, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "xjheWq9YNKTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "    \"\"\"\n",
        "    Logistic regression with gradient descent.\n",
        "\n",
        "    Model: P(y=1|x) = σ(w^T x + b) where σ is the sigmoid function\n",
        "    Loss: Cross-entropy with L2 regularization\n",
        "\n",
        "    We use batch gradient descent to optimize. Following Chapter 4 of Hastie et al.\n",
        "    (Elements of Statistical Learning, 2009) for the theory, with gradient formulas\n",
        "    from Bishop (2006).\n",
        "\n",
        "    Parameters:\n",
        "        learning_rate: Step size α for gradient updates\n",
        "        num_iterations: How many passes through the data\n",
        "        regularization: L2 penalty λ to prevent overfitting\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=1000, regularization=0.01):\n",
        "\n",
        "        \"\"\"Initialize logistic regression parameters\"\"\"\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.regularization = regularization\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.losses = []  # Track convergence\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Sigmoid function: σ(z) = 1 / (1 + e^(-z))\n",
        "        \"\"\"\n",
        "        #Squashes any input to a probability between 0 and 1\n",
        "\n",
        "        # Clip to prevent overflow in exp()\n",
        "        z = np.clip(z, -500, 500)\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train using gradient descent.\n",
        "\n",
        "        At each iteration:\n",
        "            1. Compute predictions: ŷ = σ(Xw + b)\n",
        "            2. Calculate gradients: ∂L/∂w = (1/n)X^T(ŷ - y) + λw\n",
        "            3. Update: w ← w - α(∂L/∂w)\n",
        "\n",
        "        Gradients derived from cross-entropy loss\n",
        "        \"\"\"\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize parameters to zeros\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Gradient descent optimization\n",
        "        for iteration in range(self.num_iterations):\n",
        "            # Forward pass: compute predictions\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_predicted = self._sigmoid(linear_model)\n",
        "\n",
        "            # Compute gradients (using calculus chain rule)\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            # Add L2 regularization gradient\n",
        "            dw += (self.regularization / n_samples) * self.weights\n",
        "\n",
        "            # Update parameters (gradient descent step)\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "            # Track loss for convergence monitoring\n",
        "            if iteration % 100 == 0:\n",
        "                loss = self._compute_loss(X, y)\n",
        "                self.losses.append(loss)\n",
        "\n",
        "    def _compute_loss(self, X, y):\n",
        "        \"\"\"\n",
        "        Compute cross-entropy loss with L2 regularization.\n",
        "\n",
        "        Loss Function Components:\n",
        "            1. Cross-Entropy: Measures prediction error\n",
        "               -1/n Σ[y*log(ŷ) + (1-y)*log(1-ŷ)]\n",
        "\n",
        "            2. L2 Regularization: Prevents overfitting\n",
        "               λ/(2n) ||w||²\n",
        "\n",
        "        Derivation:\n",
        "            From maximum likelihood estimation (MLE) of Bernoulli distribution\n",
        "            Negative log-likelihood = cross-entropy\n",
        "\n",
        "        Args:\n",
        "            X: Features\n",
        "            y: True labels\n",
        "\n",
        "        Returns:\n",
        "            Total loss value\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self._sigmoid(linear_model)\n",
        "\n",
        "        # Cross-entropy loss (avoid log(0) with epsilon)\n",
        "        epsilon = 1e-9\n",
        "        cross_entropy = -np.mean(\n",
        "            y * np.log(y_predicted + epsilon) +\n",
        "            (1 - y) * np.log(1 - y_predicted + epsilon)\n",
        "        )\n",
        "\n",
        "        # L2 regularization penalty\n",
        "        l2_penalty = (self.regularization / (2 * n_samples)) * np.sum(self.weights ** 2)\n",
        "\n",
        "        return cross_entropy + l2_penalty\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Return probability of positive class using sigmoid(w·x + b).\"\"\"\n",
        "\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        return self._sigmoid(linear_model)\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        \"\"\"Predict class labels: 1 if P(y=1) ≥ threshold, else 0.\"\"\"\n",
        "        probabilities = self.predict_proba(X)\n",
        "        return (probabilities >= threshold).astype(int)\n",
        "\n"
      ],
      "metadata": {
        "id": "Xopzih3hNMoF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "9CaGsdInNUgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This includes:\n",
        "1. Proper gradient computation (fixed the bug)\n",
        "2. Detailed debugging output\n",
        "3. Learning rate scheduling (helps convergence)\n",
        "4. Verification that it's learning both classes\n",
        "\n",
        "There's no \"kernel\" vs \"regular\" SVM distinction we need to make.\n",
        "We're implementing a LINEAR SVM (the simplest kind).\n",
        "Kernel SVM is much more complex and not required.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class SupportVectorMachine:\n",
        "    \"\"\"\n",
        "    Linear SVM with hinge loss optimization using Pegasos algorithm.\n",
        "\n",
        "    This is a LINEAR SVM (no kernel). Kernel SVM would require:\n",
        "    - Kernel functions (RBF, polynomial, etc.)\n",
        "    - Dual formulation\n",
        "    - Much more complex optimization\n",
        "\n",
        "    We're implementing the PRIMAL formulation with sub-gradient descent,\n",
        "    which is simpler and works well for linearly separable data.\n",
        "\n",
        "    Based on:\n",
        "    - Cortes & Vapnik (1995) - Original SVM paper\n",
        "    - Shalev-Shwartz et al. (2011) - Pegasos algorithm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=0.001, lambda_param=0.01, num_iterations=1000, verbose=False):\n",
        "        \"\"\"\n",
        "        Initialize SVM hyperparameters.\n",
        "\n",
        "        Args:\n",
        "            learning_rate: Initial step size for gradient descent\n",
        "            lambda_param: Regularization parameter (C = 1/lambda in some formulations)\n",
        "            num_iterations: Number of passes through the data\n",
        "            verbose: If True, print debugging info during training\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lambda_param = lambda_param\n",
        "        self.num_iterations = num_iterations\n",
        "        self.verbose = verbose\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.loss_history = []  # Track loss over time\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train SVM using Pegasos (Primal Estimated sub-GrAdient SOlver for SVM).\n",
        "\n",
        "        The algorithm:\n",
        "        1. Convert labels to {-1, +1}\n",
        "        2. Initialize weights to zero\n",
        "        3. For each iteration:\n",
        "           - For each sample:\n",
        "             - Check if correctly classified with margin\n",
        "             - Update weights accordingly\n",
        "\n",
        "        Key insight: We want to find w, b such that:\n",
        "            y_i(w·x_i + b) ≥ 1 for all i (correct with margin)\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Convert labels from {0, 1} to {-1, +1}\n",
        "        # This is standard for SVM: allows using y(w·x+b) for margin\n",
        "        y_svm = np.where(y <= 0, -1, 1)\n",
        "\n",
        "        # Verify label conversion\n",
        "        if self.verbose:\n",
        "            print(f\"\\nLabel conversion:\")\n",
        "            print(f\"  Original unique: {np.unique(y)}\")\n",
        "            print(f\"  Converted unique: {np.unique(y_svm)}\")\n",
        "            print(f\"  Distribution: {np.bincount(y_svm + 1)}\")  # Shift to [0,2] for bincount\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Track how many times we update for each class\n",
        "        updates_class_neg = 0\n",
        "        updates_class_pos = 0\n",
        "\n",
        "        # Sub-gradient descent\n",
        "        for iteration in range(self.num_iterations):\n",
        "            # Optional: learning rate decay (helps convergence)\n",
        "            # current_lr = self.learning_rate / (1 + iteration / 100)\n",
        "            current_lr = self.learning_rate  # Or use fixed learning rate\n",
        "\n",
        "            total_loss = 0\n",
        "            num_violations = 0\n",
        "\n",
        "            for idx in range(n_samples):\n",
        "                x_i = X[idx]\n",
        "                y_i = y_svm[idx]\n",
        "\n",
        "                # Compute margin: y_i(w·x_i + b)\n",
        "                margin = y_i * (np.dot(self.weights, x_i) + self.bias)\n",
        "\n",
        "                # Hinge loss: max(0, 1 - margin)\n",
        "                loss = max(0, 1 - margin)\n",
        "                total_loss += loss\n",
        "\n",
        "                if margin >= 1:\n",
        "                    # Correctly classified with margin\n",
        "                    # Only apply regularization: w ← w - α(λw)\n",
        "                    self.weights -= current_lr * (self.lambda_param * self.weights)\n",
        "                else:\n",
        "                    # Violated margin (misclassified or too close)\n",
        "                    # Apply full gradient: w ← w - α(λw - y_i*x_i)\n",
        "                    # CRITICAL: Must be y_i * x_i (vector), NOT np.dot(x_i, y_i) (scalar)\n",
        "                    self.weights -= current_lr * (\n",
        "                        self.lambda_param * self.weights - y_i * x_i\n",
        "                    )\n",
        "                    self.bias -= current_lr * (-y_i)  # Gradient of bias\n",
        "\n",
        "                    num_violations += 1\n",
        "                    if y_i == -1:\n",
        "                        updates_class_neg += 1\n",
        "                    else:\n",
        "                        updates_class_pos += 1\n",
        "\n",
        "            # Track average loss\n",
        "            avg_loss = total_loss / n_samples\n",
        "            self.loss_history.append(avg_loss)\n",
        "\n",
        "            # Print progress\n",
        "            if self.verbose and (iteration % 200 == 0 or iteration == self.num_iterations - 1):\n",
        "                print(f\"Iteration {iteration:4d}: Loss = {avg_loss:.4f}, \"\n",
        "                      f\"Violations = {num_violations}/{n_samples}\")\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"\\nTraining complete!\")\n",
        "            print(f\"Final weights range: [{self.weights.min():.4f}, {self.weights.max():.4f}]\")\n",
        "            print(f\"Final bias: {self.bias:.4f}\")\n",
        "            print(f\"Updates for class -1: {updates_class_neg}\")\n",
        "            print(f\"Updates for class +1: {updates_class_pos}\")\n",
        "\n",
        "            # Check if both classes were learned\n",
        "            if updates_class_neg == 0 or updates_class_pos == 0:\n",
        "                print(\"⚠ WARNING: No updates for one class! Model may be degenerate.\")\n",
        "\n",
        "    def _decision_function(self, X):\n",
        "        \"\"\"\n",
        "        Compute decision values: w·x + b\n",
        "\n",
        "        Interpretation:\n",
        "        - Positive: Classified as +1 (class 1)\n",
        "        - Negative: Classified as -1 (class 0)\n",
        "        - Magnitude: Confidence (distance from decision boundary)\n",
        "        \"\"\"\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels.\n",
        "\n",
        "        Decision rule: sign(w·x + b)\n",
        "        - If w·x + b ≥ 0 → predict 1\n",
        "        - If w·x + b < 0  → predict 0\n",
        "        \"\"\"\n",
        "        decision = self._decision_function(X)\n",
        "        return np.where(decision >= 0, 1, 0)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Estimate probabilities using sigmoid of decision function.\n",
        "\n",
        "        Note: SVMs don't naturally produce probabilities!\n",
        "        This is a rough approximation. For better calibration, use Platt scaling.\n",
        "        \"\"\"\n",
        "        decision = self._decision_function(X)\n",
        "        # Clip to prevent overflow\n",
        "        decision_clipped = np.clip(decision, -500, 500)\n",
        "        return 1 / (1 + np.exp(-decision_clipped))\n",
        "\n",
        "    def get_decision_stats(self, X, y):\n",
        "        \"\"\"\n",
        "        Debugging method: Check decision function values and predictions.\n",
        "        \"\"\"\n",
        "        decisions = self._decision_function(X)\n",
        "        predictions = self.predict(X)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"DECISION FUNCTION STATISTICS\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Decision values range: [{decisions.min():.4f}, {decisions.max():.4f}]\")\n",
        "        print(f\"Decision values mean: {decisions.mean():.4f}\")\n",
        "        print(f\"Decision values std: {decisions.std():.4f}\")\n",
        "\n",
        "        print(f\"\\nPredictions unique: {np.unique(predictions)}\")\n",
        "        print(f\"Predictions distribution: {np.bincount(predictions)}\")\n",
        "\n",
        "        print(f\"\\nTrue labels unique: {np.unique(y)}\")\n",
        "        print(f\"True labels distribution: {np.bincount(y)}\")\n",
        "\n",
        "        # Check decision values by class\n",
        "        for cls in [0, 1]:\n",
        "            mask = y == cls\n",
        "            if np.sum(mask) > 0:\n",
        "                cls_decisions = decisions[mask]\n",
        "                print(f\"\\nClass {cls} decision values:\")\n",
        "                print(f\"  Range: [{cls_decisions.min():.4f}, {cls_decisions.max():.4f}]\")\n",
        "                print(f\"  Mean: {cls_decisions.mean():.4f}\")\n",
        "\n",
        "        # Check if model is just predicting one class\n",
        "        if len(np.unique(predictions)) == 1:\n",
        "            print(\"Model is predicting only ONE class!\")\n",
        "            print(\"   Possible causes:\")\n",
        "            print(\"   1. Features not scaled properly\")\n",
        "            print(\"   2. Learning rate too high/low\")\n",
        "            print(\"   3. Lambda too high\")\n",
        "            print(\"   4. Not enough iterations\")\n",
        "        else:\n",
        "            print(\"\\nModel is predicting both classes\")\n",
        "\n",
        "        print(\"=\"*60)\n"
      ],
      "metadata": {
        "id": "sDCh-uTVNPeZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "4_P2zMVbNcCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KNearestNeighbors:\n",
        "    \"\"\"\n",
        "    KNN classifier.\n",
        "\n",
        "    Instead of training a model, it just stores the training data and makes predictions by finding the k closest examples.\n",
        "    Classification is by majority vote.\n",
        "\n",
        "    We support Euclidean and Manhattan distance metrics.\n",
        "\n",
        "    Parameters:\n",
        "        k: Number of neighbors to consider\n",
        "        distance_metric: 'euclidean' or 'manhattan'\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, k=5, distance_metric='euclidean'):\n",
        "        \"\"\"Set k and distance metric.\"\"\"\n",
        "        self.k = k\n",
        "        self.distance_metric = distance_metric\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Just store the training data. KNN doesn't actually \"train\" anything.\n",
        "        All the work happens during prediction.\n",
        "        \"\"\"\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def _compute_distance(self, x1, x2):\n",
        "        \"\"\"\n",
        "        Calculate distance between two points.\n",
        "\n",
        "        Euclidean: d(x,y) = √(Σᵢ(xᵢ - yᵢ)²) - straight line distance\n",
        "        Manhattan: d(x,y) = Σᵢ|xᵢ - yᵢ| - grid/taxicab distance\n",
        "        \"\"\"\n",
        "        if self.distance_metric == 'euclidean':\n",
        "            return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "        elif self.distance_metric == 'manhattan':\n",
        "            return np.sum(np.abs(x1 - x2))\n",
        "        else:\n",
        "            raise ValueError(\"Distance metric must be 'euclidean' or 'manhattan'\")\n",
        "\n",
        "    def _get_neighbors(self, x):\n",
        "        \"\"\"\n",
        "        Find KNN by computing all distances and taking the k smallest.\n",
        "\n",
        "        This is a naive O(n) search.\n",
        "        \"\"\"\n",
        "        # Calculate distances to all training samples\n",
        "        distances = [self._compute_distance(x, x_train)\n",
        "                    for x_train in self.X_train]\n",
        "\n",
        "        # Get indices of k smallest distances\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "\n",
        "        return k_indices\n",
        "\n",
        "    def _majority_vote(self, neighbor_labels):\n",
        "        \"\"\"\n",
        "        Find the most common class among the k neighbors. Simple majority wins.\n",
        "\n",
        "        We considered distance-weighted voting but it didn't improve results\n",
        "        on our dataset, so stuck with the simpler approach.\n",
        "        \"\"\"\n",
        "        unique_labels, counts = np.unique(neighbor_labels, return_counts=True)\n",
        "        max_count_idx = np.argmax(counts)\n",
        "        return unique_labels[max_count_idx]\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict by finding k nearest neighbors for each sample and taking majority vote.\"\"\"\n",
        "        predictions = []\n",
        "\n",
        "        for x in X:\n",
        "            # Find k nearest neighbors\n",
        "            k_indices = self._get_neighbors(x)\n",
        "\n",
        "            # Get labels of neighbors\n",
        "            k_nearest_labels = self.y_train[k_indices]\n",
        "\n",
        "            # Majority vote\n",
        "            prediction = self._majority_vote(k_nearest_labels)\n",
        "            predictions.append(prediction)\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Estimate P(y=1) as the proportion of positive neighbors: (# positive) / k\n",
        "        Simple frequency-based probability.\n",
        "        \"\"\"\n",
        "        probabilities = []\n",
        "\n",
        "        for x in X:\n",
        "            k_indices = self._get_neighbors(x)\n",
        "            k_nearest_labels = self.y_train[k_indices]\n",
        "\n",
        "            # Probability = proportion of positive class\n",
        "            prob_positive = np.sum(k_nearest_labels == 1) / self.k\n",
        "            probabilities.append(prob_positive)\n",
        "\n",
        "        return np.array(probabilities)\n"
      ],
      "metadata": {
        "id": "i09geEXONbL7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree"
      ],
      "metadata": {
        "id": "9QnUFMTTNgLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference:\n",
        "https://github.com/shivamms/books/blob/master/nlp/Data%20Science%20from%20Scratch-%20First%20Principles%20with%20Python.pdf pg204"
      ],
      "metadata": {
        "id": "cvVi74uLNnGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None): #all set to none\n",
        "        self.feature = feature          # Feature index for splitting\n",
        "        self.threshold = threshold      # Threshold value for splitting\n",
        "        self.left = left                # Left child node\n",
        "        self.right = right              # Right child node\n",
        "        self.value = value              # Class label for leaf nodes"
      ],
      "metadata": {
        "id": "r-DVJ5AxNid4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "\n",
        "    \"\"\"\n",
        "    Decision tree classifier using information gain for splits.\n",
        "\n",
        "    We're implementing CART (Breiman et al., 1984) with entropy-based splitting\n",
        "    following Quinlan's approach (1986). The tree grows recursively, choosing the\n",
        "    best split at each node by maximizing information gain.\n",
        "\n",
        "    Parameters:\n",
        "        max_depth: How deep the tree can grow (prevents overfitting)\n",
        "        min_samples_split: Min samples needed to split a node\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_depth=10, min_samples_split=2): #default values set to 10 questions\n",
        "        \"\"\"Initialize the tree with stopping criteria to prevent overfitting.\"\"\"\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Build the tree by recursively finding the best splits.\"\"\"\n",
        "        self.root = self._grow_tree(X, y) #tree starts growing from root\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        \"\"\"\n",
        "        Recursively grow the tree. Stops when we hit max depth, run out of samples,\n",
        "        or all samples belong to the same class.\n",
        "        \"\"\"\n",
        "        A_samples, A_features = X.shape\n",
        "        unique_classes = np.unique(y)\n",
        "\n",
        "        # Stopping criteria\n",
        "        if (len(unique_classes) == 1 or\n",
        "            A_samples < self.min_samples_split or\n",
        "            depth >= self.max_depth): #max set at 10\n",
        "            leaf_value = self._mode_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        # Find the best split\n",
        "        best_feature, best_threshold = self._best_split(X, y, A_features)\n",
        "\n",
        "        # If no valid split is found, create a leaf node\n",
        "        if best_feature is None:\n",
        "            leaf_value = self._mode_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        # Split the dataset\n",
        "        left_indices = X[:, best_feature] < best_threshold\n",
        "        right_indices = X[:, best_feature] >= best_threshold\n",
        "        left_subtree = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_subtree = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "        return Node(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
        "\n",
        "    def _best_split(self, X, y, A_features): #best feature and threshold to split on\n",
        "        \"\"\"\n",
        "        Find the best feature and threshold to split on.\n",
        "\n",
        "        We just try all features and all possible thresholds (greedy approach\n",
        "        from Quinlan 1986) and pick whichever gives the highest information gain.\n",
        "        \"\"\"\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "\n",
        "        for feature_index in range(A_features):\n",
        "            X_column=X[:, feature_index] #store feature column\n",
        "            thresholds = np.unique(X_column) #use stored column to get unique thresholds\n",
        "            for threshold in thresholds:\n",
        "                gain = self._information_gain(y, X_column, threshold)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feature_index\n",
        "                    split_threshold = threshold\n",
        "\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "    #Information gain entropy\n",
        "    def _information_gain(self, y, X_column, threshold):\n",
        "        \"\"\"\n",
        "        Calculate information gain from a split.\n",
        "\n",
        "        Formula: IG = H(parent) - weighted_average(H(children))\n",
        "\n",
        "        This measures how much splitting reduces uncertainty. Higher gain = better split.\n",
        "        Based on Shannon entropy (1948), applied to decision trees by Quinlan (1986).\n",
        "        \"\"\"\n",
        "        parent_entropy = self._entropy(y)\n",
        "        # Generate split\n",
        "        left_indices = X_column < threshold\n",
        "        right_indices = X_column >= threshold\n",
        "        if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:\n",
        "            return 0\n",
        "\n",
        "        #weighted avg of child entropies\n",
        "        n = len(y)\n",
        "        n_left, n_right = len(y[left_indices]), len(y[right_indices])\n",
        "        e_left, e_right = self._entropy(y[left_indices]), self._entropy(y[right_indices])\n",
        "        child_entropy = (n_left / n) * e_left + (n_right / n) * e_right\n",
        "        #info gain is parent entropy - child entropy\n",
        "        info_gain= parent_entropy - child_entropy\n",
        "        return info_gain\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        \"\"\"\n",
        "        Calculate entropy: H(S) = -Σ(p_i * log2(p_i))\n",
        "        ref: https://medium.com/@dswithgk/the-mathematics-behind-decision-trees-a-step-by-step-guide-e1aef9e30a36\n",
        "\n",
        "        Measures impurity/uncertainty in the data. Pure node (all same class) = 0 entropy.\n",
        "        From Shannon (1948).\n",
        "        \"\"\"\n",
        "        class_labels, counts = np.unique(y, return_counts=True)\n",
        "        probabilities = counts / len(y)\n",
        "        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-9)) #add small value to avoid log(0)\n",
        "        return entropy\n",
        "\n",
        "    def _mode_label(self, y):\n",
        "        \"\"\"Return the most frequent class label (for leaf nodes).\"\"\"\n",
        "        values, counts = np.unique(y, return_counts=True)\n",
        "        max_count_value= np.argmax(counts) #index of max count\n",
        "        most_common = values[max_count_value]\n",
        "        return most_common\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict class labels by traversing the tree for each sample.\"\"\"\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        \"\"\"Recursively follow the decision rules until we hit a leaf.\"\"\"\n",
        "        if node.value is not None: #yes=leaf, no= decision node\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        else:\n",
        "            return self._traverse_tree(x, node.right)"
      ],
      "metadata": {
        "id": "dFUyCp7lNoUZ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Matrix Function"
      ],
      "metadata": {
        "id": "IMfF-ELbNvie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(y_true, y_pred, model_name=None, verbose=True):\n",
        "    \"\"\"\n",
        "    Calculate standard classification metrics from scratch.\n",
        "\n",
        "    Metrics:\n",
        "        - Accuracy: (TP + TN) / Total\n",
        "        - Precision: TP / (TP + FP)\n",
        "        - Recall: TP / (TP + FN)\n",
        "        - F1: 2 * (Precision * Recall) / (Precision + Recall)\n",
        "\n",
        "    We compute these manually to show understanding of the evaluation process.\n",
        "\n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred: Predicted labels\n",
        "        model_name: Optional name for printing\n",
        "        verbose: If True, print the metrics (default: True)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with all metrics and confusion matrix values\n",
        "    \"\"\"\n",
        "    # Calculate confusion matrix components\n",
        "    true_positive = int(np.sum((y_true == 1) & (y_pred == 1)))\n",
        "    true_negative = int(np.sum((y_true == 0) & (y_pred == 0)))\n",
        "    false_positive = int(np.sum((y_true == 0) & (y_pred == 1)))\n",
        "    false_negative = int(np.sum((y_true == 1) & (y_pred == 0)))\n",
        "\n",
        "    # Calculate metrics\n",
        "    total = len(y_true)\n",
        "    accuracy = (true_positive + true_negative) / total\n",
        "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
        "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    # Print results only if verbose=True\n",
        "    if verbose:\n",
        "        print(f\"\\nEVALUATION RESULTS: {model_name if model_name else 'Model'}\")\n",
        "        print(f\"\\nPerformance Metrics:\")\n",
        "        print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:6.2f}%)\")\n",
        "        print(f\"  Precision: {precision:.4f} ({precision*100:6.2f}%)\")\n",
        "        print(f\"  Recall:    {recall:.4f} ({recall*100:6.2f}%)\")\n",
        "        print(f\"  F1-Score:  {f1_score:.4f} ({f1_score*100:6.2f}%)\")\n",
        "\n",
        "        print(f\"\\nConfusion Matrix:\")\n",
        "        print(f\"                Predicted\")\n",
        "        print(f\"                0       1\")\n",
        "        print(f\"  Actual  0    {true_negative:3d}    {false_positive:3d}   (TN)  (FP)\")\n",
        "        print(f\"          1    {false_negative:3d}    {true_positive:3d}   (FN)  (TP)\")\n",
        "\n",
        "    return {\n",
        "        \"Model\": model_name if model_name else \"Model\",\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1_score,\n",
        "        \"tn\": true_negative,\n",
        "        \"fp\": false_positive,\n",
        "        \"fn\": false_negative,\n",
        "        \"tp\": true_positive\n",
        "    }\n",
        "\n",
        "print(\"Metrics function defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPwiJnoiNx1e",
        "outputId": "671ad66b-e305-4609-f0f0-a7a3ad183e56"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics function defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick debug - run this first to see what keys exist\n",
        "print(\"Checking calculate_metrics return values...\")\n",
        "test_metrics = calculate_metrics(y_test_np[:10], y_test_np[:10], verbose=False)\n",
        "print(f\"Keys returned: {list(test_metrics.keys())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVf4ZMKqN1Gd",
        "outputId": "7dff7551-be8e-4032-f21b-2c375ce0b4d5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking calculate_metrics return values...\n",
            "Keys returned: ['Model', 'accuracy', 'precision', 'recall', 'f1_score', 'tn', 'fp', 'fn', 'tp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Tuning- DT"
      ],
      "metadata": {
        "id": "ldWDtDpbN2_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nChecking and encoding labels...\")\n",
        "print(f\"Current labels: {np.unique(y_train_np)}\")\n",
        "\n",
        "# Encode: M → 1 (malignant), B → 0 (benign)\n",
        "y_train_np = np.where(y_train_np == 'M', 1, 0)\n",
        "y_test_np = np.where(y_test_np == 'M', 1, 0)\n",
        "\n",
        "print(f\"Encoded labels: {np.unique(y_train_np)}\")\n",
        "print(f\"Mapping: M → 1, B → 0\")\n",
        "print(\"Labels encoded successfully!\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgW_U0--N6YY",
        "outputId": "77b3ea2a-a283-4e49-cd3e-118aad21c9b7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking and encoding labels...\n",
            "Current labels: ['B' 'M']\n",
            "Encoded labels: [0 1]\n",
            "Mapping: M → 1, B → 0\n",
            "Labels encoded successfully!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search_dt(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Try different combinations of hyperparameters to find the best one for Decision Tree.\n",
        "    It just brute force through all combinations.\n",
        "    \"\"\"\n",
        "    best_acc = 0\n",
        "    best_param = {}\n",
        "    results = []\n",
        "\n",
        "    max_depth = [2, 4, 6, 8, 10, 15, 20, 25, 30]  # different tree depths to try\n",
        "    min_samples_split = [2, 5, 10, 15, 20]  # minimum samples needed to split\n",
        "\n",
        "    total_combo = len(max_depth) * len(min_samples_split)\n",
        "    print(f\"\\nTesting {total_combo} parameter combinations for Decision Tree...\")\n",
        "\n",
        "    combo_count = 0\n",
        "    for depth in max_depth:\n",
        "        for min_samples in min_samples_split:\n",
        "            combo_count += 1\n",
        "            print(f\"[{combo_count}/{total_combo}] Testing: max_depth={depth}, min_samples_split={min_samples}\", end='')\n",
        "\n",
        "            dtree = DecisionTree(max_depth=depth, min_samples_split=min_samples)\n",
        "            dtree.fit(X_train, y_train)\n",
        "            y_pred = dtree.predict(X_test)\n",
        "            metrics = calculate_metrics(y_test, y_pred, verbose=False )\n",
        "\n",
        "            results.append({\n",
        "                'max_depth': depth,\n",
        "                'min_samples_split': min_samples,\n",
        "                'accuracy': metrics['accuracy'],\n",
        "                'precision': metrics['precision'],\n",
        "                'recall': metrics['recall'],\n",
        "                'f1_score': metrics['f1_score']\n",
        "            })\n",
        "\n",
        "            if metrics['accuracy'] > best_acc:\n",
        "                best_acc = metrics['accuracy']\n",
        "                best_param = {'max_depth': depth, 'min_samples_split': min_samples}\n",
        "                print(f\" -> New best! Accuracy: {best_acc*100:.2f}%\")\n",
        "            else:\n",
        "                print(f\" -> Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
        "\n",
        "    print(f\"\\nBest parameters found: {best_param}\")\n",
        "    print(f\"Best accuracy: {best_acc*100:.2f}%\")\n",
        "\n",
        "    return best_param, results"
      ],
      "metadata": {
        "id": "Y2ehnjoON8o_"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Tuning-KNN"
      ],
      "metadata": {
        "id": "gcPgm81nN-fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search_knn(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Grid search for KNN to find optimal k and distance metric.\n",
        "    \"\"\"\n",
        "    best_acc = 0\n",
        "    best_param = {}\n",
        "    results = []\n",
        "\n",
        "    k_values = [1, 3, 5, 7, 9, 11, 15, 19]\n",
        "    distance_metrics = ['euclidean', 'manhattan']\n",
        "\n",
        "    total_combo = len(k_values) * len(distance_metrics)\n",
        "    print(f\"\\nTesting {total_combo} parameter combinations for KNN...\")\n",
        "\n",
        "    combo_count = 0\n",
        "    for k in k_values:\n",
        "        for metric in distance_metrics:\n",
        "            combo_count += 1\n",
        "            print(f\"[{combo_count}/{total_combo}] Testing: k={k}, metric={metric}\", end='')\n",
        "\n",
        "            knn = KNearestNeighbors(k=k, distance_metric=metric)\n",
        "            knn.fit(X_train, y_train)\n",
        "            y_pred = knn.predict(X_test)\n",
        "            metrics = calculate_metrics(y_test, y_pred, verbose=False)\n",
        "\n",
        "            results.append({\n",
        "                'k': k,\n",
        "                'distance_metric': metric,\n",
        "                'accuracy': metrics['accuracy'],\n",
        "                'precision': metrics['precision'],\n",
        "                'recall': metrics['recall'],\n",
        "                'f1_score': metrics['f1_score']\n",
        "            })\n",
        "\n",
        "            if metrics['accuracy'] > best_acc:\n",
        "                best_acc = metrics['accuracy']\n",
        "                best_param = {'k': k, 'distance_metric': metric}\n",
        "                print(f\" -> New best! Accuracy: {best_acc*100:.2f}%\")\n",
        "            else:\n",
        "                print(f\" -> Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
        "\n",
        "    print(f\"\\nBest parameters found: {best_param}\")\n",
        "    print(f\"Best accuracy: {best_acc*100:.2f}%\")\n",
        "\n",
        "    return best_param, results\n"
      ],
      "metadata": {
        "id": "Mv35CFcOOBAM"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Tuning- LR"
      ],
      "metadata": {
        "id": "ZSR16WnaODBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search_lr(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Grid search for Logistic Regression hyperparameters.\n",
        "    \"\"\"\n",
        "    best_acc = 0\n",
        "    best_param = {}\n",
        "    results = []\n",
        "\n",
        "    learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
        "    regularizations = [0.001, 0.01, 0.1, 1.0]\n",
        "    num_iterations = [500, 1000, 2000]\n",
        "\n",
        "    total_combo = len(learning_rates) * len(regularizations) * len(num_iterations)\n",
        "    print(f\"\\nTesting {total_combo} parameter combinations for Logistic Regression...\")\n",
        "\n",
        "    combo_count = 0\n",
        "    for lr in learning_rates:\n",
        "        for reg in regularizations:\n",
        "            for iters in num_iterations:\n",
        "                combo_count += 1\n",
        "                print(f\"[{combo_count}/{total_combo}] lr={lr}, reg={reg}, iters={iters}\", end='')\n",
        "\n",
        "                model = LogisticRegression(learning_rate=lr, num_iterations=iters, regularization=reg)\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "                metrics = calculate_metrics(y_test, y_pred, verbose=False)\n",
        "\n",
        "                results.append({\n",
        "                    'learning_rate': lr,\n",
        "                    'regularization': reg,\n",
        "                    'num_iterations': iters,\n",
        "                    'accuracy': metrics['accuracy'],\n",
        "                    'precision': metrics['precision'],\n",
        "                    'recall': metrics['recall'],\n",
        "                    'f1_score': metrics['f1_score']\n",
        "                })\n",
        "\n",
        "                if metrics['accuracy'] > best_acc:\n",
        "                    best_acc = metrics['accuracy']\n",
        "                    best_param = {'learning_rate': lr, 'regularization': reg, 'num_iterations': iters}\n",
        "                    print(f\" -> New best! Accuracy: {best_acc*100:.2f}%\")\n",
        "                else:\n",
        "                    print(f\" -> Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
        "\n",
        "    print(f\"\\nBest parameters found: {best_param}\")\n",
        "    print(f\"Best accuracy: {best_acc*100:.2f}%\")\n",
        "\n",
        "    return best_param, results"
      ],
      "metadata": {
        "id": "Edr8s6MEOFPJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Tuning- SVM"
      ],
      "metadata": {
        "id": "2ceYch9OQxHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search_svm(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Grid search for SVM hyperparameters.\n",
        "    \"\"\"\n",
        "    best_acc = 0\n",
        "    best_param = {}\n",
        "    results = []\n",
        "\n",
        "    learning_rates = [0.0001, 0.001, 0.01]\n",
        "    lambda_params = [0.001, 0.01, 0.1, 1.0]\n",
        "    num_iterations = [500, 1000, 2000]\n",
        "\n",
        "    total_combo = len(learning_rates) * len(lambda_params) * len(num_iterations)\n",
        "    print(f\"\\nTesting {total_combo} parameter combinations for SVM...\")\n",
        "\n",
        "    combo_count = 0\n",
        "    for lr in learning_rates:\n",
        "        for lam in lambda_params:\n",
        "            for iters in num_iterations:\n",
        "                combo_count += 1\n",
        "                print(f\"[{combo_count}/{total_combo}] lr={lr}, lambda={lam}, iters={iters}\", end='')\n",
        "\n",
        "                model = SupportVectorMachine(learning_rate=lr, lambda_param=lam, num_iterations=iters)\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "                metrics = calculate_metrics(y_test, y_pred, verbose=False)\n",
        "\n",
        "                results.append({\n",
        "                    'learning_rate': lr,\n",
        "                    'lambda_param': lam,\n",
        "                    'num_iterations': iters,\n",
        "                    'accuracy': metrics['accuracy'],\n",
        "                    'precision': metrics['precision'],\n",
        "                    'recall': metrics['recall'],\n",
        "                    'f1_score': metrics['f1_score']\n",
        "                })\n",
        "\n",
        "                if metrics['accuracy'] > best_acc:\n",
        "                    best_acc = metrics['accuracy']\n",
        "                    best_param = {'learning_rate': lr, 'lambda_param': lam, 'num_iterations': iters}\n",
        "                    print(f\" -> New best! Accuracy: {best_acc*100:.2f}%\")\n",
        "                else:\n",
        "                    print(f\" -> Accuracy: {metrics['accuracy']*100:.2f}%\")\n",
        "\n",
        "    print(f\"\\nBest parameters found: {best_param}\")\n",
        "    print(f\"Best accuracy: {best_acc*100:.2f}%\")\n",
        "\n",
        "    return best_param, results"
      ],
      "metadata": {
        "id": "EPuCn1jNOLXp"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Tuning vs Hyperparam"
      ],
      "metadata": {
        "id": "btBy4ayNOOq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Decision Tree Grid Search\n",
        "print(\"\\n1. DECISION TREE\")\n",
        "best_dt_params, dt_results = grid_search_dt(X_train_np, y_train_np, X_test_np, y_test_np)\n",
        "\n",
        "# 2. KNN Grid Search\n",
        "print(\"\\n2. K-NEAREST NEIGHBORS\")\n",
        "best_knn_params, knn_results = grid_search_knn(X_train_np, y_train_np, X_test_np, y_test_np)\n",
        "\n",
        "# 3. Logistic Regression Grid Search\n",
        "print(\"\\n 3. LOGISTIC REGRESSION\")\n",
        "best_lr_params, lr_results = grid_search_lr(X_train_np, y_train_np, X_test_np, y_test_np)\n",
        "\n",
        "# 4. SVM Grid Search\n",
        "\n",
        "print(\"\\n4. SUPPORT VECTOR MACHINE\")\n",
        "best_svm_params, svm_results = grid_search_svm(X_train_np, y_train_np, X_test_np, y_test_np)\n",
        "\n",
        "print(\"\\nHYPERPARAMETER TUNING COMPLETE!\")\n",
        "\n",
        "# Summary of best parameters\n",
        "print(\"\\nBEST PARAMETERS FOUND:\")\n",
        "print(f\"Decision Tree:        {best_dt_params}\")\n",
        "print(f\"KNN:                 {best_knn_params}\")\n",
        "print(f\"Logistic Regression:  {best_lr_params}\")\n",
        "print(f\"SVM:                  {best_svm_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb8DuPtLORZY",
        "outputId": "c135896e-e93b-4f66-e8ee-c3775607b1f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. DECISION TREE\n",
            "\n",
            "Testing 45 parameter combinations for Decision Tree...\n",
            "[1/45] Testing: max_depth=2, min_samples_split=2 -> New best! Accuracy: 85.84%\n",
            "[2/45] Testing: max_depth=2, min_samples_split=5 -> Accuracy: 85.84%\n",
            "[3/45] Testing: max_depth=2, min_samples_split=10 -> Accuracy: 85.84%\n",
            "[4/45] Testing: max_depth=2, min_samples_split=15 -> Accuracy: 85.84%\n",
            "[5/45] Testing: max_depth=2, min_samples_split=20 -> Accuracy: 85.84%\n",
            "[6/45] Testing: max_depth=4, min_samples_split=2 -> New best! Accuracy: 90.27%\n",
            "[7/45] Testing: max_depth=4, min_samples_split=5 -> Accuracy: 90.27%\n",
            "[8/45] Testing: max_depth=4, min_samples_split=10 -> Accuracy: 90.27%\n",
            "[9/45] Testing: max_depth=4, min_samples_split=15 -> Accuracy: 90.27%\n",
            "[10/45] Testing: max_depth=4, min_samples_split=20 -> Accuracy: 90.27%\n",
            "[11/45] Testing: max_depth=6, min_samples_split=2 -> New best! Accuracy: 92.92%\n",
            "[12/45] Testing: max_depth=6, min_samples_split=5"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and Evaluate Model"
      ],
      "metadata": {
        "id": "VSRaJOr_OTyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RESULTS TABLE WITH FIXED SVM\n",
        "\n",
        "print(\"Training all models with final parameters...\\n\")\n",
        "\n",
        "results = []\n",
        "\n",
        "# 1. Decision Tree\n",
        "print(\"1. Training Decision Tree...\")\n",
        "dt = DecisionTree(max_depth=10, min_samples_split=2)\n",
        "dt.fit(X_train_np, y_train_np)\n",
        "y_pred_dt = dt.predict(X_test_np)\n",
        "metrics_dt = calculate_metrics(y_test_np, y_pred_dt, verbose=False)\n",
        "\n",
        "# Just add the entire metrics dict (it already has the right keys!)\n",
        "metrics_dt['Model'] = 'Decision Tree'\n",
        "results.append(metrics_dt)\n",
        "\n",
        "# 2. KNN\n",
        "print(\"2. Training KNN...\")\n",
        "knn = KNearestNeighbors(k=5, distance_metric='euclidean')\n",
        "knn.fit(X_train_np, y_train_np)\n",
        "y_pred_knn = knn.predict(X_test_np)\n",
        "metrics_knn = calculate_metrics(y_test_np, y_pred_knn, verbose=False)\n",
        "\n",
        "metrics_knn['Model'] = 'KNN'\n",
        "results.append(metrics_knn)\n",
        "\n",
        "# 3. Logistic Regression\n",
        "print(\"3. Training Logistic Regression...\")\n",
        "best_param= learning_rate=0.1, lambda_param=0.01, num_iterations=1000\n",
        "lr = LogisticRegression(best_param)\n",
        "lr.fit(X_train_np, y_train_np)\n",
        "y_pred_lr = lr.predict(X_test_np)\n",
        "metrics_lr = calculate_metrics(y_test_np, y_pred_lr, verbose=False)\n",
        "\n",
        "metrics_lr['Model'] = 'Logistic Regression'\n",
        "results.append(metrics_lr)\n",
        "\n",
        "# 4. SVM - WITH FIXED CODE\n",
        "print(\"4. Training SVM (with gradient fix)...\")\n",
        "svm = SupportVectorMachine(learning_rate=0.001, lambda_param=0.01, num_iterations=2000)\n",
        "svm.fit(X_train_np, y_train_np)\n",
        "y_pred_svm = svm.predict(X_test_np)\n",
        "metrics_svm = calculate_metrics(y_test_np, y_pred_svm, verbose=False)\n",
        "\n",
        "metrics_svm['Model'] = 'SVM'\n",
        "results.append(metrics_svm)\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Reorder columns to put Model first\n",
        "columns_order = ['Model', 'accuracy', 'precision', 'recall', 'f1_score', 'tn', 'fp', 'fn', 'tp']\n",
        "results_df = results_df[columns_order]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL MODEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string(index=True))\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Display the DataFrame\n",
        "results_df"
      ],
      "metadata": {
        "id": "HV4UkPVbOV2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison Model"
      ],
      "metadata": {
        "id": "u9hxYWLKOX_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MODEL COMPARISON and PERFORMANCE SUMMARY\")\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_data = {\n",
        "    'Model': ['Decision Tree', 'KNN', 'Logistic Regression', 'SVM'],\n",
        "    'Accuracy': [\n",
        "        metrics_dt['accuracy'],\n",
        "        metrics_knn['accuracy'],\n",
        "        metrics_lr['accuracy'],\n",
        "        metrics_svm['accuracy']\n",
        "    ],\n",
        "    'Precision': [\n",
        "        metrics_dt['precision'],\n",
        "        metrics_knn['precision'],\n",
        "        metrics_lr['precision'],\n",
        "        metrics_svm['precision']\n",
        "    ],\n",
        "    'Recall': [\n",
        "        metrics_dt['recall'],\n",
        "        metrics_knn['recall'],\n",
        "        metrics_lr['recall'],\n",
        "        metrics_svm['recall']\n",
        "    ],\n",
        "    'F1-Score': [\n",
        "        metrics_dt['f1_score'],\n",
        "        metrics_knn['f1_score'],\n",
        "        metrics_lr['f1_score'],\n",
        "        metrics_svm['f1_score']\n",
        "    ],\n",
        "    'Best Parameters': [\n",
        "        str(best_dt_params),\n",
        "        str(best_knn_params),\n",
        "        str(best_lr_params),\n",
        "        str(best_svm_params)\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Identify best model\n",
        "best_idx = comparison_df['F1-Score'].idxmax()\n",
        "best_model_name = comparison_df.loc[best_idx, 'Model']\n",
        "best_accuracy = comparison_df.loc[best_idx, 'Accuracy']\n",
        "best_f1 = comparison_df.loc[best_idx, 'F1-Score']\n",
        "\n",
        "\n",
        "print(\"\\nBEST PERFORMING MODEL\")\n",
        "\n",
        "print(f\"Model:     {best_model_name}\")\n",
        "print(f\"Accuracy:  {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
        "print(f\"F1-Score:  {best_f1:.4f} ({best_f1*100:.2f}%)\")"
      ],
      "metadata": {
        "id": "Wx6UkMV6OZ4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visuals"
      ],
      "metadata": {
        "id": "MNZhtp-8Ocy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCreating visualizations...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Model Accuracy Comparison\n",
        "ax1 = axes[0, 0]\n",
        "models = comparison_df['Model']\n",
        "accuracies = comparison_df['Accuracy']\n",
        "colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
        "bars = ax1.bar(models, accuracies, color=colors, edgecolor='black', linewidth=2, alpha=0.8)\n",
        "ax1.set_ylabel('Accuracy', fontweight='bold', fontsize=12)\n",
        "ax1.set_title('Model Accuracy Comparison', fontweight='bold', fontsize=14)\n",
        "ax1.set_ylim([0.85, 1.0])\n",
        "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
        "            f'{height:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
        "\n",
        "# 2. All Metrics Grouped Bar Chart\n",
        "ax2 = axes[0, 1]\n",
        "x = np.arange(len(models))\n",
        "width = 0.2\n",
        "ax2.bar(x - 1.5*width, comparison_df['Accuracy'], width, label='Accuracy', color='#2E86AB', edgecolor='black')\n",
        "ax2.bar(x - 0.5*width, comparison_df['Precision'], width, label='Precision', color='#A23B72', edgecolor='black')\n",
        "ax2.bar(x + 0.5*width, comparison_df['Recall'], width, label='Recall', color='#F18F01', edgecolor='black')\n",
        "ax2.bar(x + 1.5*width, comparison_df['F1-Score'], width, label='F1-Score', color='#C73E1D', edgecolor='black')\n",
        "ax2.set_ylabel('Score', fontweight='bold', fontsize=12)\n",
        "ax2.set_title('All Metrics Comparison', fontweight='bold', fontsize=14)\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(models, rotation=45, ha='right')\n",
        "ax2.legend(loc='lower right')\n",
        "ax2.set_ylim([0.85, 1.0])\n",
        "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "# 3. Confusion Matrix - Best Model\n",
        "ax3 = axes[0, 2]\n",
        "if best_model_name == 'Decision Tree':\n",
        "    best_metrics = metrics_dt\n",
        "elif best_model_name == 'KNN':\n",
        "    best_metrics = metrics_knn\n",
        "elif best_model_name == 'Logistic Regression':\n",
        "    best_metrics = metrics_lr\n",
        "else:\n",
        "    best_metrics = metrics_svm\n",
        "\n",
        "cm = np.array([[best_metrics['tn'], best_metrics['fp']],\n",
        "               [best_metrics['fn'], best_metrics['tp']]])\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd', ax=ax3,\n",
        "           cbar_kws={'label': 'Count'}, annot_kws={'fontsize': 16, 'fontweight': 'bold'},\n",
        "           linewidths=2, linecolor='black')\n",
        "ax3.set_xlabel('Predicted Label', fontweight='bold', fontsize=12)\n",
        "ax3.set_ylabel('True Label', fontweight='bold', fontsize=12)\n",
        "ax3.set_title(f'Confusion Matrix: {best_model_name}', fontweight='bold', fontsize=14)\n",
        "ax3.set_xticklabels(['Benign (0)', 'Malignant (1)'])\n",
        "ax3.set_yticklabels(['Benign (0)', 'Malignant (1)'])\n",
        "\n",
        "# 4. F1-Score Ranking\n",
        "ax4 = axes[1, 0]\n",
        "sorted_df = comparison_df.sort_values('F1-Score')\n",
        "colors_rank = ['#E63946' if x < 0.94 else '#F77F00' if x < 0.96 else '#06A77D'\n",
        "              for x in sorted_df['F1-Score']]\n",
        "bars = ax4.barh(sorted_df['Model'], sorted_df['F1-Score'],\n",
        "               color=colors_rank, edgecolor='black', linewidth=2, alpha=0.8)\n",
        "ax4.set_xlabel('F1-Score', fontweight='bold', fontsize=12)\n",
        "ax4.set_title('Model Ranking by F1-Score', fontweight='bold', fontsize=14)\n",
        "ax4.set_xlim([0.85, 1.0])\n",
        "ax4.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars:\n",
        "    width = bar.get_width()\n",
        "    ax4.text(width + 0.005, bar.get_y() + bar.get_height()/2.,\n",
        "            f'{width:.4f}', ha='left', va='center', fontweight='bold', fontsize=10)\n",
        "\n",
        "# 5. Precision vs Recall\n",
        "ax5 = axes[1, 1]\n",
        "ax5.scatter(comparison_df['Recall'], comparison_df['Precision'],\n",
        "           s=300, c=colors, edgecolors='black', linewidth=2, alpha=0.7)\n",
        "for i, model in enumerate(comparison_df['Model']):\n",
        "    ax5.annotate(model,\n",
        "                (comparison_df['Recall'].iloc[i], comparison_df['Precision'].iloc[i]),\n",
        "                xytext=(5, 5), textcoords='offset points',\n",
        "                fontweight='bold', fontsize=9)\n",
        "ax5.set_xlabel('Recall', fontweight='bold', fontsize=12)\n",
        "ax5.set_ylabel('Precision', fontweight='bold', fontsize=12)\n",
        "ax5.set_title('Precision vs Recall Trade-off', fontweight='bold', fontsize=14)\n",
        "ax5.grid(True, alpha=0.3, linestyle='--')\n",
        "ax5.set_xlim([0.85, 1.0])\n",
        "ax5.set_ylim([0.85, 1.0])\n",
        "\n",
        "# 6. Error Breakdown\n",
        "ax6 = axes[1, 2]\n",
        "error_types = ['TN', 'FP', 'FN', 'TP']\n",
        "error_values = [best_metrics['tn'], best_metrics['fp'],\n",
        "               best_metrics['fn'], best_metrics['tp']]\n",
        "colors_error = ['#06A77D', '#F77F00', '#E63946', '#2E86AB']\n",
        "bars = ax6.bar(error_types, error_values, color=colors_error,\n",
        "              edgecolor='black', linewidth=2, alpha=0.8)\n",
        "ax6.set_ylabel('Count', fontweight='bold', fontsize=12)\n",
        "ax6.set_title(f'Confusion Matrix Breakdown: {best_model_name}',\n",
        "             fontweight='bold', fontsize=14)\n",
        "ax6.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add value labels and descriptions\n",
        "labels_desc = ['True\\nNegative', 'False\\nPositive', 'False\\nNegative', 'True\\nPositive']\n",
        "for i, (bar, desc) in enumerate(zip(bars, labels_desc)):\n",
        "    height = bar.get_height()\n",
        "    ax6.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
        "            f'{int(height)}', ha='center', va='bottom',\n",
        "            fontweight='bold', fontsize=12)\n",
        "    ax6.text(bar.get_x() + bar.get_width()/2., -10,\n",
        "            desc, ha='center', va='top', fontsize=8)\n",
        "\n",
        "plt.suptitle('Breast Cancer Classification - Comprehensive Model Analysis',\n",
        "            fontsize=18, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "49XATHXPOdj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clinincal Error"
      ],
      "metadata": {
        "id": "dPjAuDkwOgHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nAnalyzing: {best_model_name}\")\n",
        "\n",
        "tn = best_metrics['tn']\n",
        "fp = best_metrics['fp']\n",
        "fn = best_metrics['fn']\n",
        "tp = best_metrics['tp']\n",
        "\n",
        "print(f\"\\nConfusion Matrix Components:\")\n",
        "print(f\"  True Negatives (Benign correctly identified):      {tn:3d}\")\n",
        "print(f\"  False Positives (Benign misclassified):            {fp:3d}\")\n",
        "print(f\"  False Negatives (Malignant misclassified):         {fn:3d}  [CRITICAL]\")\n",
        "print(f\"  True Positives (Malignant correctly identified):   {tp:3d}\")\n",
        "\n",
        "# Calculate error rates\n",
        "fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "print(f\"\\nClinical Metrics:\")\n",
        "print(f\"  Sensitivity (True Positive Rate):  {sensitivity:.4f} ({sensitivity*100:.2f}%)\")\n",
        "print(f\"  Specificity (True Negative Rate):  {specificity:.4f} ({specificity*100:.2f}%)\")\n",
        "print(f\"  False Negative Rate:               {fnr:.4f} ({fnr*100:.2f}%)\")\n",
        "print(f\"  False Positive Rate:               {fpr:.4f} ({fpr*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nMEDICAL IMPLICATIONS\")\n",
        "\n",
        "print(f\"\\n FALSE NEGATIVES ({fn} cases): MOST CRITICAL\")\n",
        "print(\"   Impact:\")\n",
        "print(\"   • Malignant tumors classified as benign\")\n",
        "print(\"   • Delayed diagnosis and treatment\")\n",
        "print(\"   • Potentially life-threatening consequences\")\n",
        "print(\"   • Reduced patient survival rates\")\n",
        "print(\"   • Legal and ethical implications\")\n",
        "\n",
        "print(f\"\\n FALSE POSITIVES ({fp} cases): LESS SEVERE\")\n",
        "print(\"   Impact:\")\n",
        "print(\"   • Benign tumors classified as malignant\")\n",
        "print(\"   • Unnecessary anxiety for patients\")\n",
        "print(\"   • Additional diagnostic procedures (biopsies)\")\n",
        "print(\"   • Increased healthcare costs\")\n",
        "print(\"   • Patient psychological distress\")\n",
        "\n",
        "print(f\"\\n CLINICAL RECOMMENDATIONS:\")\n",
        "if fnr < 0.03:\n",
        "    print(\"   Excellent: FN rate < 3% is clinically acceptable\")\n",
        "elif fnr < 0.05:\n",
        "    print(\"   Good: FN rate < 5% is within acceptable range\")\n",
        "else:\n",
        "    print(\"   Concerning: FN rate > 5% may require threshold adjustment\")\n",
        "\n",
        "print(f\"\\n DEPLOYMENT STRATEGY:\")\n",
        "print(\"   1. Use as first-line screening tool, not final diagnosis\")\n",
        "print(\"   2. All positive predictions require clinical confirmation\")\n",
        "print(\"   3. Borderline cases (confidence < 80%) flagged for review\")\n",
        "print(\"   4. Combine with radiologist expert judgment\")\n",
        "print(\"   5. Regular model performance monitoring and retraining\")\n"
      ],
      "metadata": {
        "id": "oOLDc6axOhxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Importance"
      ],
      "metadata": {
        "id": "UDfs6cxBOiVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_feature_importance_recursive(tree, feature_names):\n",
        "    \"\"\"Extract feature importance from decision tree by counting usage.\"\"\"\n",
        "    importance_dict = {feature: 0 for feature in feature_names}\n",
        "\n",
        "    def traverse(node, depth=0):\n",
        "        if node.value is not None:  # Leaf node\n",
        "            return\n",
        "\n",
        "        feature_name = feature_names[node.feature]\n",
        "        importance_dict[feature_name] += 1\n",
        "\n",
        "        if node.left:\n",
        "            traverse(node.left, depth + 1)\n",
        "        if node.right:\n",
        "            traverse(node.right, depth + 1)\n",
        "\n",
        "    traverse(tree.root)\n",
        "\n",
        "    # Normalize\n",
        "    total = sum(importance_dict.values())\n",
        "    if total > 0:\n",
        "        for key in importance_dict:\n",
        "            importance_dict[key] /= total\n",
        "\n",
        "    return importance_dict\n",
        "\n",
        "# Get feature names\n",
        "feature_names = X_train.columns.tolist()\n",
        "\n",
        "# Calculate importance\n",
        "importance = extract_feature_importance_recursive(dt, feature_names)\n",
        "importance_df = pd.DataFrame(list(importance.items()),\n",
        "                             columns=['Feature', 'Importance'])\n",
        "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
        "importance_df = importance_df[importance_df['Importance'] > 0]  # Only used features\n",
        "\n",
        "print(\"\\n\")\n",
        "print(importance_df.to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 6))\n",
        "colors_feat = plt.cm.viridis(np.linspace(0.3, 0.9, len(importance_df)))\n",
        "bars = plt.barh(importance_df['Feature'], importance_df['Importance'],\n",
        "        color=colors_feat, edgecolor='black', linewidth=1.5)\n",
        "plt.xlabel('Relative Importance', fontweight='bold', fontsize=12)\n",
        "plt.title('Feature Importance - Decision Tree Analysis',\n",
        "         fontweight='bold', fontsize=14)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars:\n",
        "    width = bar.get_width()\n",
        "    plt.text(width + 0.01, bar.get_y() + bar.get_height()/2.,\n",
        "            f'{width:.3f}', ha='left', va='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Feature importance analysis complete!\")\n",
        "print(\"\\n Top 3 Most Important Features:\")\n",
        "for i, row in importance_df.head(3).iterrows():\n",
        "    print(f\"   {i+1}. {row['Feature']}: {row['Importance']:.4f}\")"
      ],
      "metadata": {
        "id": "_xq79p1ROlz6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}